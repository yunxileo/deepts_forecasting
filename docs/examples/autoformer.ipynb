{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Informer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install deepts_forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from deepts_forecasting.utils.data import TimeSeriesDataSet\n",
    "from deepts_forecasting.utils.data.encoders import TorchNormalizer\n",
    "from deepts_forecasting.datasets import AirPassengersDataset\n",
    "from deepts_forecasting.models.autoformer import Autoformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       Month  Passengers  year month group  time_idx\n0 1949-01-01       112.0  1949     1     0         0\n1 1949-02-01       118.0  1949     2     0         1\n2 1949-03-01       132.0  1949     3     0         2\n3 1949-04-01       129.0  1949     4     0         3\n4 1949-05-01       121.0  1949     5     0         4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Month</th>\n      <th>Passengers</th>\n      <th>year</th>\n      <th>month</th>\n      <th>group</th>\n      <th>time_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1949-01-01</td>\n      <td>112.0</td>\n      <td>1949</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1949-02-01</td>\n      <td>118.0</td>\n      <td>1949</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1949-03-01</td>\n      <td>132.0</td>\n      <td>1949</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1949-04-01</td>\n      <td>129.0</td>\n      <td>1949</td>\n      <td>4</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1949-05-01</td>\n      <td>121.0</td>\n      <td>1949</td>\n      <td>5</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = AirPassengersDataset().load()\n",
    "data['year'] = data['Month'].dt.year\n",
    "data['month'] = data['Month'].dt.month\n",
    "data['group'] = '0'\n",
    "data['time_idx'] = np.arange(len(data))\n",
    "data['Passengers'] = data['Passengers'].astype(float)\n",
    "data['month'] = data['month'].astype('str')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_encoder_length = 18\n",
    "max_prediction_length = 12\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_encoder_length - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    max_encoder_length= max_encoder_length,\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Passengers\",\n",
    "    group_ids=[\"group\"],\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=['month'],\n",
    "    time_varying_known_reals=[],\n",
    "    time_varying_unknown_reals=[\"Passengers\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    target_normalizer=TorchNormalizer(method=\"standard\",\n",
    "                                      transformation=None),\n",
    "    )\n",
    "\n",
    "training.get_parameters()\n",
    "validation = TimeSeriesDataSet.from_dataset(training,\n",
    "                                            data[lambda x: x.time_idx > training_cutoff])\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(training, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "val_dataloader = DataLoader(validation, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": "<bound method LightningModule.summarize of Autoformer(\n  (loss): L1Loss()\n  (logging_metrics): ModuleList()\n  (decomp): SeriesDecompose(\n    (moving_avg): MovingAvg(\n      (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n    )\n  )\n  (embeddings): ModuleDict(\n    (month): Embedding(12, 6)\n  )\n  (encoder_input_linear): TokenEmbedding(\n    (tokenConv): Conv1d(7, 28, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n  )\n  (decoder_input_linear): TokenEmbedding(\n    (tokenConv): Conv1d(6, 28, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n  )\n  (out_linear): Linear(in_features=28, out_features=1, bias=True)\n  (encoder): Encoder(\n    (attn_layers): ModuleList(\n      (0): EncoderLayer(\n        (attention): AutoCorrelationLayer(\n          (inner_correlation): AutoCorrelation(\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (query_projection): Linear(in_features=28, out_features=28, bias=True)\n          (key_projection): Linear(in_features=28, out_features=28, bias=True)\n          (value_projection): Linear(in_features=28, out_features=28, bias=True)\n          (out_projection): Linear(in_features=28, out_features=28, bias=True)\n        )\n        (conv1): Conv1d(28, 56, kernel_size=(1,), stride=(1,), bias=False)\n        (conv2): Conv1d(56, 28, kernel_size=(1,), stride=(1,), bias=False)\n        (decomp1): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (decomp2): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): EncoderLayer(\n        (attention): AutoCorrelationLayer(\n          (inner_correlation): AutoCorrelation(\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (query_projection): Linear(in_features=28, out_features=28, bias=True)\n          (key_projection): Linear(in_features=28, out_features=28, bias=True)\n          (value_projection): Linear(in_features=28, out_features=28, bias=True)\n          (out_projection): Linear(in_features=28, out_features=28, bias=True)\n        )\n        (conv1): Conv1d(28, 56, kernel_size=(1,), stride=(1,), bias=False)\n        (conv2): Conv1d(56, 28, kernel_size=(1,), stride=(1,), bias=False)\n        (decomp1): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (decomp2): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): MyLayerNorm(\n      (layernorm): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (decoder): Decoder(\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (self_attention): AutoCorrelationLayer(\n          (inner_correlation): AutoCorrelation(\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (query_projection): Linear(in_features=28, out_features=28, bias=True)\n          (key_projection): Linear(in_features=28, out_features=28, bias=True)\n          (value_projection): Linear(in_features=28, out_features=28, bias=True)\n          (out_projection): Linear(in_features=28, out_features=28, bias=True)\n        )\n        (cross_attention): AutoCorrelationLayer(\n          (inner_correlation): AutoCorrelation(\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (query_projection): Linear(in_features=28, out_features=28, bias=True)\n          (key_projection): Linear(in_features=28, out_features=28, bias=True)\n          (value_projection): Linear(in_features=28, out_features=28, bias=True)\n          (out_projection): Linear(in_features=28, out_features=28, bias=True)\n        )\n        (conv1): Conv1d(28, 56, kernel_size=(1,), stride=(1,), bias=False)\n        (conv2): Conv1d(56, 28, kernel_size=(1,), stride=(1,), bias=False)\n        (decomp1): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (decomp2): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (decomp3): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (projection): Conv1d(28, 28, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n      )\n      (1): DecoderLayer(\n        (self_attention): AutoCorrelationLayer(\n          (inner_correlation): AutoCorrelation(\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (query_projection): Linear(in_features=28, out_features=28, bias=True)\n          (key_projection): Linear(in_features=28, out_features=28, bias=True)\n          (value_projection): Linear(in_features=28, out_features=28, bias=True)\n          (out_projection): Linear(in_features=28, out_features=28, bias=True)\n        )\n        (cross_attention): AutoCorrelationLayer(\n          (inner_correlation): AutoCorrelation(\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (query_projection): Linear(in_features=28, out_features=28, bias=True)\n          (key_projection): Linear(in_features=28, out_features=28, bias=True)\n          (value_projection): Linear(in_features=28, out_features=28, bias=True)\n          (out_projection): Linear(in_features=28, out_features=28, bias=True)\n        )\n        (conv1): Conv1d(28, 56, kernel_size=(1,), stride=(1,), bias=False)\n        (conv2): Conv1d(56, 28, kernel_size=(1,), stride=(1,), bias=False)\n        (decomp1): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (decomp2): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (decomp3): SeriesDecompose(\n          (moving_avg): MovingAvg(\n            (avg): AvgPool1d(kernel_size=(12,), stride=(1,), padding=(0,))\n          )\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (projection): Conv1d(28, 28, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n      )\n    )\n    (norm): MyLayerNorm(\n      (layernorm): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n)>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(1234)\n",
    "# create PyTorch Lighning Trainer with early stopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4,\n",
    "                                    patience=60, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    gpus=0,  # run on CPU, if on multiple GPUs, use accelerator=\"ddp\"\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # 30 batches per epoch\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\")\n",
    ")\n",
    "\n",
    "model = Autoformer.from_dataset(training,\n",
    "                                    d_model=28,\n",
    "                                    d_ff=56,\n",
    "                                    n_heads=2,\n",
    "                                    num_layers=2,\n",
    "                                    moving_avg=12,\n",
    "                                      )\n",
    "model.summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"activation\":                        relu\n\"categorical_groups\":                {}\n\"d_ff\":                              56\n\"d_model\":                           28\n\"dropout\":                           0.1\n\"embedding_labels\":                  {'month': array(['1', '10', '11', '12', '2', '3', '4', '5', '6', '7', '8', '9'],\n      dtype=object)}\n\"embedding_paddings\":                []\n\"embedding_sizes\":                   {'month': [12, 6]}\n\"factor\":                            5\n\"learning_rate\":                     0.001\n\"log_interval\":                      -1\n\"log_val_interval\":                  None\n\"logging_metrics\":                   ModuleList()\n\"loss\":                              L1Loss()\n\"max_encoder_length\":                18\n\"max_prediction_length\":             12\n\"monotone_constaints\":               {}\n\"moving_avg\":                        12\n\"n_heads\":                           2\n\"num_layers\":                        2\n\"output_size\":                       1\n\"output_transformer\":                TorchNormalizer()\n\"static_categoricals\":               []\n\"static_reals\":                      []\n\"time_varying_categoricals_decoder\": ['month']\n\"time_varying_categoricals_encoder\": ['month']\n\"time_varying_reals_decoder\":        []\n\"time_varying_reals_encoder\":        ['Passengers']\n\"x_categoricals\":                    ['month']\n\"x_reals\":                           ['Passengers']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name                 | Type            | Params\n",
      "---------------------------------------------------------\n",
      "0 | loss                 | L1Loss          | 0     \n",
      "1 | logging_metrics      | ModuleList      | 0     \n",
      "2 | decomp               | SeriesDecompose | 0     \n",
      "3 | embeddings           | ModuleDict      | 72    \n",
      "4 | encoder_input_linear | TokenEmbedding  | 588   \n",
      "5 | decoder_input_linear | TokenEmbedding  | 504   \n",
      "6 | out_linear           | Linear          | 29    \n",
      "7 | encoder              | Encoder         | 12.8 K\n",
      "8 | decoder              | Decoder         | 24.0 K\n",
      "---------------------------------------------------------\n",
      "38.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "38.0 K    Total params\n",
      "0.152     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 1234\n",
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:432: UserWarning: The number of training samples (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  86%|████████▌ | 6/7 [00:00<00:00, 11.97it/s, loss=1.23, v_num=24] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 7/7 [00:00<00:00, 13.06it/s, loss=1.23, v_num=24, val_loss=2.990]\n",
      "Epoch 1:  86%|████████▌ | 6/7 [00:00<00:00,  9.66it/s, loss=1.19, v_num=24, val_loss=2.990, train_loss=1.120]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 7/7 [00:00<00:00, 10.57it/s, loss=1.19, v_num=24, val_loss=5.780, train_loss=1.120]\n",
      "Epoch 2:  86%|████████▌ | 6/7 [00:00<00:00,  7.78it/s, loss=1.06, v_num=24, val_loss=5.780, train_loss=0.958]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 7/7 [00:00<00:00,  8.51it/s, loss=1.06, v_num=24, val_loss=0.821, train_loss=0.958]\n",
      "Epoch 3:  86%|████████▌ | 6/7 [00:00<00:00,  6.96it/s, loss=1.02, v_num=24, val_loss=0.821, train_loss=0.763] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 7/7 [00:00<00:00,  7.77it/s, loss=1.02, v_num=24, val_loss=2.830, train_loss=0.763]\n",
      "Epoch 4:  86%|████████▌ | 6/7 [00:00<00:00,  8.16it/s, loss=0.876, v_num=24, val_loss=2.830, train_loss=0.893]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4: 100%|██████████| 7/7 [00:00<00:00,  9.08it/s, loss=0.876, v_num=24, val_loss=4.130, train_loss=0.893]\n",
      "Epoch 5:  86%|████████▌ | 6/7 [00:00<00:00,  8.89it/s, loss=0.816, v_num=24, val_loss=4.130, train_loss=0.631]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 5: 100%|██████████| 7/7 [00:00<00:00,  9.84it/s, loss=0.816, v_num=24, val_loss=1.370, train_loss=0.631]\n",
      "Epoch 6:  86%|████████▌ | 6/7 [00:00<00:00,  9.79it/s, loss=0.715, v_num=24, val_loss=1.370, train_loss=0.674]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6: 100%|██████████| 7/7 [00:00<00:00, 11.00it/s, loss=0.715, v_num=24, val_loss=1.640, train_loss=0.674]\n",
      "Epoch 7:  86%|████████▌ | 6/7 [00:00<00:00, 10.59it/s, loss=0.714, v_num=24, val_loss=1.640, train_loss=0.529]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 7/7 [00:00<00:00, 11.82it/s, loss=0.714, v_num=24, val_loss=2.800, train_loss=0.529]\n",
      "Epoch 8:  86%|████████▌ | 6/7 [00:00<00:00, 11.40it/s, loss=0.742, v_num=24, val_loss=2.800, train_loss=0.717]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 7/7 [00:00<00:00, 12.67it/s, loss=0.742, v_num=24, val_loss=1.950, train_loss=0.717]\n",
      "Epoch 9:  86%|████████▌ | 6/7 [00:00<00:00, 11.76it/s, loss=0.752, v_num=24, val_loss=1.950, train_loss=0.547]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 9: 100%|██████████| 7/7 [00:00<00:00, 13.03it/s, loss=0.752, v_num=24, val_loss=2.240, train_loss=0.547]\n",
      "Epoch 10:  86%|████████▌ | 6/7 [00:00<00:00, 12.41it/s, loss=0.849, v_num=24, val_loss=2.240, train_loss=0.791]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10: 100%|██████████| 7/7 [00:00<00:00, 13.60it/s, loss=0.849, v_num=24, val_loss=3.090, train_loss=0.791]\n",
      "Epoch 11:  86%|████████▌ | 6/7 [00:00<00:00, 10.81it/s, loss=1, v_num=24, val_loss=3.090, train_loss=0.891]    \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 11: 100%|██████████| 7/7 [00:00<00:00, 12.04it/s, loss=1, v_num=24, val_loss=0.645, train_loss=0.891]\n",
      "Epoch 12:  86%|████████▌ | 6/7 [00:00<00:00, 10.88it/s, loss=1.08, v_num=24, val_loss=0.645, train_loss=1.150] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 12: 100%|██████████| 7/7 [00:00<00:00, 12.02it/s, loss=1.08, v_num=24, val_loss=1.740, train_loss=1.150]\n",
      "Epoch 13:  86%|████████▌ | 6/7 [00:00<00:00, 12.70it/s, loss=1.06, v_num=24, val_loss=1.740, train_loss=0.840] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 13: 100%|██████████| 7/7 [00:00<00:00, 14.10it/s, loss=1.06, v_num=24, val_loss=2.350, train_loss=0.840]\n",
      "Epoch 14:  86%|████████▌ | 6/7 [00:00<00:00, 12.18it/s, loss=1.09, v_num=24, val_loss=2.350, train_loss=0.835]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14: 100%|██████████| 7/7 [00:00<00:00, 13.49it/s, loss=1.09, v_num=24, val_loss=1.990, train_loss=0.835]\n",
      "Epoch 15:  86%|████████▌ | 6/7 [00:00<00:00, 12.36it/s, loss=1.1, v_num=24, val_loss=1.990, train_loss=1.120] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 15: 100%|██████████| 7/7 [00:00<00:00, 13.78it/s, loss=1.1, v_num=24, val_loss=5.380, train_loss=1.120]\n",
      "Epoch 16:  86%|████████▌ | 6/7 [00:00<00:00, 12.22it/s, loss=1.06, v_num=24, val_loss=5.380, train_loss=1.040]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 16: 100%|██████████| 7/7 [00:00<00:00, 13.51it/s, loss=1.06, v_num=24, val_loss=3.980, train_loss=1.040]\n",
      "Epoch 17:  86%|████████▌ | 6/7 [00:00<00:00, 12.78it/s, loss=0.897, v_num=24, val_loss=3.980, train_loss=0.556]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17: 100%|██████████| 7/7 [00:00<00:00, 14.17it/s, loss=0.897, v_num=24, val_loss=1.440, train_loss=0.556]\n",
      "Epoch 18:  86%|████████▌ | 6/7 [00:00<00:00, 11.93it/s, loss=0.876, v_num=24, val_loss=1.440, train_loss=0.719]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 18: 100%|██████████| 7/7 [00:00<00:00, 13.26it/s, loss=0.876, v_num=24, val_loss=3.420, train_loss=0.719]\n",
      "Epoch 19:  86%|████████▌ | 6/7 [00:00<00:00, 12.74it/s, loss=0.908, v_num=24, val_loss=3.420, train_loss=1.000]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 14.21it/s, loss=0.908, v_num=24, val_loss=2.180, train_loss=1.000]\n",
      "Epoch 20:  86%|████████▌ | 6/7 [00:00<00:00, 12.96it/s, loss=0.906, v_num=24, val_loss=2.180, train_loss=0.857]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 20: 100%|██████████| 7/7 [00:00<00:00, 14.39it/s, loss=0.906, v_num=24, val_loss=0.780, train_loss=0.857]\n",
      "Epoch 21:  86%|████████▌ | 6/7 [00:00<00:00, 12.93it/s, loss=0.81, v_num=24, val_loss=0.780, train_loss=0.682] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 21: 100%|██████████| 7/7 [00:00<00:00, 14.31it/s, loss=0.81, v_num=24, val_loss=2.160, train_loss=0.682]\n",
      "Epoch 22:  86%|████████▌ | 6/7 [00:00<00:00, 13.11it/s, loss=0.778, v_num=24, val_loss=2.160, train_loss=0.721]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 22: 100%|██████████| 7/7 [00:00<00:00, 14.55it/s, loss=0.778, v_num=24, val_loss=3.110, train_loss=0.721]\n",
      "Epoch 23:  86%|████████▌ | 6/7 [00:00<00:00, 13.07it/s, loss=0.74, v_num=24, val_loss=3.110, train_loss=0.637] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23: 100%|██████████| 7/7 [00:00<00:00, 14.51it/s, loss=0.74, v_num=24, val_loss=3.700, train_loss=0.637]\n",
      "Epoch 24:  86%|████████▌ | 6/7 [00:00<00:00, 12.66it/s, loss=0.696, v_num=24, val_loss=3.700, train_loss=0.477]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24: 100%|██████████| 7/7 [00:00<00:00, 13.94it/s, loss=0.696, v_num=24, val_loss=3.580, train_loss=0.477]\n",
      "Epoch 25:  86%|████████▌ | 6/7 [00:00<00:00, 11.37it/s, loss=0.682, v_num=24, val_loss=3.580, train_loss=0.737]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 25: 100%|██████████| 7/7 [00:00<00:00, 12.62it/s, loss=0.682, v_num=24, val_loss=3.410, train_loss=0.737]\n",
      "Epoch 26:  86%|████████▌ | 6/7 [00:00<00:00, 11.67it/s, loss=0.859, v_num=24, val_loss=3.410, train_loss=0.544]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 26: 100%|██████████| 7/7 [00:00<00:00, 13.02it/s, loss=0.859, v_num=24, val_loss=0.840, train_loss=0.544]\n",
      "Epoch 27:  86%|████████▌ | 6/7 [00:00<00:00, 14.17it/s, loss=0.792, v_num=24, val_loss=0.840, train_loss=1.040]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 27: 100%|██████████| 7/7 [00:00<00:00, 15.69it/s, loss=0.792, v_num=24, val_loss=1.180, train_loss=1.040]\n",
      "Epoch 28:  86%|████████▌ | 6/7 [00:00<00:00, 14.76it/s, loss=0.88, v_num=24, val_loss=1.180, train_loss=0.644] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 28: 100%|██████████| 7/7 [00:00<00:00, 16.33it/s, loss=0.88, v_num=24, val_loss=1.640, train_loss=0.644]\n",
      "Epoch 29:  86%|████████▌ | 6/7 [00:00<00:00, 12.09it/s, loss=0.852, v_num=24, val_loss=1.640, train_loss=0.636]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 29: 100%|██████████| 7/7 [00:00<00:00, 13.32it/s, loss=0.852, v_num=24, val_loss=1.820, train_loss=0.636]\n",
      "Epoch 30:  86%|████████▌ | 6/7 [00:00<00:00, 12.54it/s, loss=0.805, v_num=24, val_loss=1.820, train_loss=0.778]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 30: 100%|██████████| 7/7 [00:00<00:00, 13.68it/s, loss=0.805, v_num=24, val_loss=1.480, train_loss=0.778]\n",
      "Epoch 31:  86%|████████▌ | 6/7 [00:00<00:00, 12.28it/s, loss=0.766, v_num=24, val_loss=1.480, train_loss=0.801]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 31: 100%|██████████| 7/7 [00:00<00:00, 13.59it/s, loss=0.766, v_num=24, val_loss=2.980, train_loss=0.801]\n",
      "Epoch 32:  86%|████████▌ | 6/7 [00:00<00:00, 12.24it/s, loss=0.735, v_num=24, val_loss=2.980, train_loss=0.506]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 32: 100%|██████████| 7/7 [00:00<00:00, 13.55it/s, loss=0.735, v_num=24, val_loss=3.710, train_loss=0.506]\n",
      "Epoch 33:  86%|████████▌ | 6/7 [00:00<00:00, 12.19it/s, loss=0.814, v_num=24, val_loss=3.710, train_loss=0.653]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 33: 100%|██████████| 7/7 [00:00<00:00, 13.66it/s, loss=0.814, v_num=24, val_loss=0.582, train_loss=0.653]\n",
      "Epoch 34:  86%|████████▌ | 6/7 [00:00<00:00, 11.91it/s, loss=0.879, v_num=24, val_loss=0.582, train_loss=1.040]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 34: 100%|██████████| 7/7 [00:00<00:00, 13.31it/s, loss=0.879, v_num=24, val_loss=1.280, train_loss=1.040]\n",
      "Epoch 35:  86%|████████▌ | 6/7 [00:00<00:00, 12.43it/s, loss=0.971, v_num=24, val_loss=1.280, train_loss=0.693]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 35: 100%|██████████| 7/7 [00:00<00:00, 13.72it/s, loss=0.971, v_num=24, val_loss=3.720, train_loss=0.693]\n",
      "Epoch 36:  86%|████████▌ | 6/7 [00:00<00:00, 11.90it/s, loss=0.975, v_num=24, val_loss=3.720, train_loss=0.839]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 36: 100%|██████████| 7/7 [00:00<00:00, 13.16it/s, loss=0.975, v_num=24, val_loss=1.990, train_loss=0.839]\n",
      "Epoch 37:  86%|████████▌ | 6/7 [00:00<00:00, 11.86it/s, loss=1.04, v_num=24, val_loss=1.990, train_loss=0.973] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 37: 100%|██████████| 7/7 [00:00<00:00, 13.14it/s, loss=1.04, v_num=24, val_loss=1.680, train_loss=0.973]\n",
      "Epoch 38:  86%|████████▌ | 6/7 [00:00<00:00, 12.41it/s, loss=1.17, v_num=24, val_loss=1.680, train_loss=1.130]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 38: 100%|██████████| 7/7 [00:00<00:00, 13.80it/s, loss=1.17, v_num=24, val_loss=1.910, train_loss=1.130]\n",
      "Epoch 39:  86%|████████▌ | 6/7 [00:00<00:00, 12.98it/s, loss=1.05, v_num=24, val_loss=1.910, train_loss=1.130]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 39: 100%|██████████| 7/7 [00:00<00:00, 14.33it/s, loss=1.05, v_num=24, val_loss=2.370, train_loss=1.130]\n",
      "Epoch 40:  86%|████████▌ | 6/7 [00:00<00:00, 12.54it/s, loss=0.984, v_num=24, val_loss=2.370, train_loss=0.713]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 40: 100%|██████████| 7/7 [00:00<00:00, 13.96it/s, loss=0.984, v_num=24, val_loss=1.630, train_loss=0.713]\n",
      "Epoch 41:  86%|████████▌ | 6/7 [00:00<00:00, 13.30it/s, loss=0.819, v_num=24, val_loss=1.630, train_loss=0.697]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 41: 100%|██████████| 7/7 [00:00<00:00, 14.77it/s, loss=0.819, v_num=24, val_loss=1.300, train_loss=0.697]\n",
      "Epoch 42:  86%|████████▌ | 6/7 [00:00<00:00, 14.61it/s, loss=0.906, v_num=24, val_loss=1.300, train_loss=0.802]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 42: 100%|██████████| 7/7 [00:00<00:00, 16.07it/s, loss=0.906, v_num=24, val_loss=1.570, train_loss=0.802]\n",
      "Epoch 43:  86%|████████▌ | 6/7 [00:00<00:00, 14.63it/s, loss=0.971, v_num=24, val_loss=1.570, train_loss=1.070]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 43: 100%|██████████| 7/7 [00:00<00:00, 16.14it/s, loss=0.971, v_num=24, val_loss=0.752, train_loss=1.070]\n",
      "Epoch 44:  86%|████████▌ | 6/7 [00:00<00:00, 14.08it/s, loss=0.926, v_num=24, val_loss=0.752, train_loss=0.693]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 44: 100%|██████████| 7/7 [00:00<00:00, 15.62it/s, loss=0.926, v_num=24, val_loss=1.120, train_loss=0.693]\n",
      "Epoch 45:  86%|████████▌ | 6/7 [00:00<00:00, 13.47it/s, loss=0.931, v_num=24, val_loss=1.120, train_loss=0.580]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 45: 100%|██████████| 7/7 [00:00<00:00, 14.92it/s, loss=0.931, v_num=24, val_loss=2.540, train_loss=0.580]\n",
      "Epoch 46:  86%|████████▌ | 6/7 [00:00<00:00, 14.51it/s, loss=0.903, v_num=24, val_loss=2.540, train_loss=0.942]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 46: 100%|██████████| 7/7 [00:00<00:00, 16.13it/s, loss=0.903, v_num=24, val_loss=2.130, train_loss=0.942]\n",
      "Epoch 47:  86%|████████▌ | 6/7 [00:00<00:00, 13.20it/s, loss=0.946, v_num=24, val_loss=2.130, train_loss=0.744]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 47: 100%|██████████| 7/7 [00:00<00:00, 14.67it/s, loss=0.946, v_num=24, val_loss=2.130, train_loss=0.744]\n",
      "Epoch 48:  86%|████████▌ | 6/7 [00:00<00:00, 11.50it/s, loss=0.773, v_num=24, val_loss=2.130, train_loss=0.694]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 48: 100%|██████████| 7/7 [00:00<00:00, 12.77it/s, loss=0.773, v_num=24, val_loss=2.050, train_loss=0.694]\n",
      "Epoch 49:  86%|████████▌ | 6/7 [00:00<00:00, 12.58it/s, loss=0.734, v_num=24, val_loss=2.050, train_loss=0.583]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 49: 100%|██████████| 7/7 [00:00<00:00, 14.04it/s, loss=0.734, v_num=24, val_loss=2.890, train_loss=0.583]\n",
      "Epoch 50:  86%|████████▌ | 6/7 [00:00<00:00, 14.94it/s, loss=0.759, v_num=24, val_loss=2.890, train_loss=0.838]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 50: 100%|██████████| 7/7 [00:00<00:00, 16.33it/s, loss=0.759, v_num=24, val_loss=2.270, train_loss=0.838]\n",
      "Epoch 51:  86%|████████▌ | 6/7 [00:00<00:00, 13.04it/s, loss=0.738, v_num=24, val_loss=2.270, train_loss=0.801]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 51: 100%|██████████| 7/7 [00:00<00:00, 14.48it/s, loss=0.738, v_num=24, val_loss=3.080, train_loss=0.801]\n",
      "Epoch 52:  86%|████████▌ | 6/7 [00:00<00:00, 11.42it/s, loss=0.697, v_num=24, val_loss=3.080, train_loss=0.482]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 52: 100%|██████████| 7/7 [00:00<00:00, 12.40it/s, loss=0.697, v_num=24, val_loss=2.680, train_loss=0.482]\n",
      "Epoch 53:  86%|████████▌ | 6/7 [00:00<00:00, 12.43it/s, loss=0.689, v_num=24, val_loss=2.680, train_loss=0.726]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 53: 100%|██████████| 7/7 [00:00<00:00, 13.90it/s, loss=0.689, v_num=24, val_loss=4.100, train_loss=0.726]\n",
      "Epoch 54:  86%|████████▌ | 6/7 [00:00<00:00,  9.92it/s, loss=0.701, v_num=24, val_loss=4.100, train_loss=0.663]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 54: 100%|██████████| 7/7 [00:00<00:00, 11.07it/s, loss=0.701, v_num=24, val_loss=2.820, train_loss=0.663]\n",
      "Epoch 55:  86%|████████▌ | 6/7 [00:00<00:00, 10.91it/s, loss=0.712, v_num=24, val_loss=2.820, train_loss=0.670]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 55: 100%|██████████| 7/7 [00:00<00:00, 12.17it/s, loss=0.712, v_num=24, val_loss=4.220, train_loss=0.670]\n",
      "Epoch 56:  86%|████████▌ | 6/7 [00:00<00:00, 13.53it/s, loss=0.731, v_num=24, val_loss=4.220, train_loss=0.822]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 56: 100%|██████████| 7/7 [00:00<00:00, 14.92it/s, loss=0.731, v_num=24, val_loss=0.705, train_loss=0.822]\n",
      "Epoch 57:  86%|████████▌ | 6/7 [00:00<00:00, 13.08it/s, loss=0.736, v_num=24, val_loss=0.705, train_loss=0.540]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 57: 100%|██████████| 7/7 [00:00<00:00, 14.61it/s, loss=0.736, v_num=24, val_loss=2.040, train_loss=0.540]\n",
      "Epoch 58:  86%|████████▌ | 6/7 [00:00<00:00, 11.40it/s, loss=0.786, v_num=24, val_loss=2.040, train_loss=0.844]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 58: 100%|██████████| 7/7 [00:00<00:00, 12.76it/s, loss=0.786, v_num=24, val_loss=2.770, train_loss=0.844]\n",
      "Epoch 59:  86%|████████▌ | 6/7 [00:00<00:00, 11.80it/s, loss=0.883, v_num=24, val_loss=2.770, train_loss=0.813]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 59: 100%|██████████| 7/7 [00:00<00:00, 13.21it/s, loss=0.883, v_num=24, val_loss=2.680, train_loss=0.813]\n",
      "Epoch 60:  86%|████████▌ | 6/7 [00:00<00:00, 11.87it/s, loss=0.874, v_num=24, val_loss=2.680, train_loss=0.726]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 60: 100%|██████████| 7/7 [00:00<00:00, 13.29it/s, loss=0.874, v_num=24, val_loss=1.730, train_loss=0.726]\n",
      "Epoch 61:  86%|████████▌ | 6/7 [00:00<00:00, 12.85it/s, loss=0.853, v_num=24, val_loss=1.730, train_loss=0.804]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 61: 100%|██████████| 7/7 [00:00<00:00, 14.30it/s, loss=0.853, v_num=24, val_loss=0.964, train_loss=0.804]\n",
      "Epoch 62:  86%|████████▌ | 6/7 [00:00<00:00, 12.64it/s, loss=0.813, v_num=24, val_loss=0.964, train_loss=0.639]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 62: 100%|██████████| 7/7 [00:00<00:00, 14.10it/s, loss=0.813, v_num=24, val_loss=3.420, train_loss=0.639]\n",
      "Epoch 63:  86%|████████▌ | 6/7 [00:00<00:00, 13.65it/s, loss=0.721, v_num=24, val_loss=3.420, train_loss=0.632]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 63: 100%|██████████| 7/7 [00:00<00:00, 14.99it/s, loss=0.721, v_num=24, val_loss=0.618, train_loss=0.632]\n",
      "Epoch 64:  86%|████████▌ | 6/7 [00:00<00:00, 11.93it/s, loss=0.735, v_num=24, val_loss=0.618, train_loss=0.562]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 64: 100%|██████████| 7/7 [00:00<00:00, 13.37it/s, loss=0.735, v_num=24, val_loss=2.180, train_loss=0.562]\n",
      "Epoch 65:  86%|████████▌ | 6/7 [00:00<00:00, 13.30it/s, loss=0.726, v_num=24, val_loss=2.180, train_loss=0.816]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 65: 100%|██████████| 7/7 [00:00<00:00, 14.69it/s, loss=0.726, v_num=24, val_loss=0.863, train_loss=0.816]\n",
      "Epoch 66:  86%|████████▌ | 6/7 [00:00<00:00, 13.45it/s, loss=0.803, v_num=24, val_loss=0.863, train_loss=0.416]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 66: 100%|██████████| 7/7 [00:00<00:00, 14.83it/s, loss=0.803, v_num=24, val_loss=2.730, train_loss=0.416]\n",
      "Epoch 67:  86%|████████▌ | 6/7 [00:00<00:00, 12.64it/s, loss=0.834, v_num=24, val_loss=2.730, train_loss=1.130]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 67: 100%|██████████| 7/7 [00:00<00:00, 14.00it/s, loss=0.834, v_num=24, val_loss=1.910, train_loss=1.130]\n",
      "Epoch 68:  86%|████████▌ | 6/7 [00:00<00:00, 13.01it/s, loss=0.889, v_num=24, val_loss=1.910, train_loss=0.906]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 68: 100%|██████████| 7/7 [00:00<00:00, 14.15it/s, loss=0.889, v_num=24, val_loss=2.780, train_loss=0.906]\n",
      "Epoch 69:  86%|████████▌ | 6/7 [00:00<00:00, 13.17it/s, loss=0.778, v_num=24, val_loss=2.780, train_loss=0.734]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 69: 100%|██████████| 7/7 [00:00<00:00, 14.58it/s, loss=0.778, v_num=24, val_loss=1.650, train_loss=0.734]\n",
      "Epoch 70:  86%|████████▌ | 6/7 [00:00<00:00, 13.73it/s, loss=0.755, v_num=24, val_loss=1.650, train_loss=0.598]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 70: 100%|██████████| 7/7 [00:00<00:00, 15.20it/s, loss=0.755, v_num=24, val_loss=0.878, train_loss=0.598]\n",
      "Epoch 71:  86%|████████▌ | 6/7 [00:00<00:00, 12.89it/s, loss=0.743, v_num=24, val_loss=0.878, train_loss=0.751]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 71: 100%|██████████| 7/7 [00:00<00:00, 14.24it/s, loss=0.743, v_num=24, val_loss=3.950, train_loss=0.751]\n",
      "Epoch 72:  86%|████████▌ | 6/7 [00:00<00:00, 13.48it/s, loss=0.768, v_num=24, val_loss=3.950, train_loss=0.768]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 72: 100%|██████████| 7/7 [00:00<00:00, 15.02it/s, loss=0.768, v_num=24, val_loss=2.490, train_loss=0.768]\n",
      "Epoch 73:  86%|████████▌ | 6/7 [00:00<00:00, 13.00it/s, loss=0.742, v_num=24, val_loss=2.490, train_loss=0.649]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 73: 100%|██████████| 7/7 [00:00<00:00, 14.49it/s, loss=0.742, v_num=24, val_loss=4.750, train_loss=0.649]\n",
      "Epoch 74:  86%|████████▌ | 6/7 [00:00<00:00, 13.70it/s, loss=0.821, v_num=24, val_loss=4.750, train_loss=0.512]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 74: 100%|██████████| 7/7 [00:00<00:00, 15.13it/s, loss=0.821, v_num=24, val_loss=2.910, train_loss=0.512]\n",
      "Epoch 75:  86%|████████▌ | 6/7 [00:00<00:00, 12.86it/s, loss=0.871, v_num=24, val_loss=2.910, train_loss=0.990]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 75: 100%|██████████| 7/7 [00:00<00:00, 14.24it/s, loss=0.871, v_num=24, val_loss=2.790, train_loss=0.990]\n",
      "Epoch 76:  86%|████████▌ | 6/7 [00:00<00:00, 12.47it/s, loss=0.933, v_num=24, val_loss=2.790, train_loss=0.815]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 76: 100%|██████████| 7/7 [00:00<00:00, 13.94it/s, loss=0.933, v_num=24, val_loss=2.410, train_loss=0.815]\n",
      "Epoch 77:  86%|████████▌ | 6/7 [00:00<00:00, 12.87it/s, loss=0.811, v_num=24, val_loss=2.410, train_loss=0.753]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 77: 100%|██████████| 7/7 [00:00<00:00, 14.30it/s, loss=0.811, v_num=24, val_loss=1.600, train_loss=0.753]\n",
      "Epoch 78:  86%|████████▌ | 6/7 [00:00<00:00, 11.34it/s, loss=0.723, v_num=24, val_loss=1.600, train_loss=0.534]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 78: 100%|██████████| 7/7 [00:00<00:00, 12.63it/s, loss=0.723, v_num=24, val_loss=2.090, train_loss=0.534]\n",
      "Epoch 79:  86%|████████▌ | 6/7 [00:00<00:00, 11.21it/s, loss=0.662, v_num=24, val_loss=2.090, train_loss=0.669]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 79: 100%|██████████| 7/7 [00:00<00:00, 12.43it/s, loss=0.662, v_num=24, val_loss=2.160, train_loss=0.669]\n",
      "Epoch 80:  86%|████████▌ | 6/7 [00:00<00:00, 12.80it/s, loss=0.69, v_num=24, val_loss=2.160, train_loss=0.665] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 80: 100%|██████████| 7/7 [00:00<00:00, 14.17it/s, loss=0.69, v_num=24, val_loss=2.730, train_loss=0.665]\n",
      "Epoch 81:  86%|████████▌ | 6/7 [00:00<00:00, 13.03it/s, loss=0.763, v_num=24, val_loss=2.730, train_loss=0.675]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 81: 100%|██████████| 7/7 [00:00<00:00, 14.46it/s, loss=0.763, v_num=24, val_loss=2.470, train_loss=0.675]\n",
      "Epoch 82:  86%|████████▌ | 6/7 [00:00<00:00, 12.90it/s, loss=0.911, v_num=24, val_loss=2.470, train_loss=0.850]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 82: 100%|██████████| 7/7 [00:00<00:00, 14.34it/s, loss=0.911, v_num=24, val_loss=1.790, train_loss=0.850]\n",
      "Epoch 83:  86%|████████▌ | 6/7 [00:00<00:00, 12.90it/s, loss=0.949, v_num=24, val_loss=1.790, train_loss=0.931]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 83: 100%|██████████| 7/7 [00:00<00:00, 14.28it/s, loss=0.949, v_num=24, val_loss=3.270, train_loss=0.931]\n",
      "Epoch 84:  86%|████████▌ | 6/7 [00:00<00:00, 12.90it/s, loss=0.913, v_num=24, val_loss=3.270, train_loss=0.752]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 84: 100%|██████████| 7/7 [00:00<00:00, 14.33it/s, loss=0.913, v_num=24, val_loss=1.330, train_loss=0.752]\n",
      "Epoch 85:  86%|████████▌ | 6/7 [00:00<00:00, 12.97it/s, loss=0.909, v_num=24, val_loss=1.330, train_loss=0.743]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 85: 100%|██████████| 7/7 [00:00<00:00, 14.39it/s, loss=0.909, v_num=24, val_loss=2.120, train_loss=0.743]\n",
      "Epoch 86:  86%|████████▌ | 6/7 [00:00<00:00, 11.83it/s, loss=0.892, v_num=24, val_loss=2.120, train_loss=0.875]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 86: 100%|██████████| 7/7 [00:00<00:00, 13.21it/s, loss=0.892, v_num=24, val_loss=2.000, train_loss=0.875]\n",
      "Epoch 87:  86%|████████▌ | 6/7 [00:00<00:00, 10.71it/s, loss=0.8, v_num=24, val_loss=2.000, train_loss=0.655]  \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 87: 100%|██████████| 7/7 [00:00<00:00, 11.84it/s, loss=0.8, v_num=24, val_loss=2.250, train_loss=0.655]\n",
      "Epoch 88:  86%|████████▌ | 6/7 [00:00<00:00, 10.43it/s, loss=0.851, v_num=24, val_loss=2.250, train_loss=0.710]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 88: 100%|██████████| 7/7 [00:00<00:00, 11.57it/s, loss=0.851, v_num=24, val_loss=0.758, train_loss=0.710]\n",
      "Epoch 89:  86%|████████▌ | 6/7 [00:00<00:00, 10.17it/s, loss=0.864, v_num=24, val_loss=0.758, train_loss=0.847]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 89: 100%|██████████| 7/7 [00:00<00:00, 11.19it/s, loss=0.864, v_num=24, val_loss=1.470, train_loss=0.847]\n",
      "Epoch 90:  86%|████████▌ | 6/7 [00:00<00:00, 13.11it/s, loss=0.846, v_num=24, val_loss=1.470, train_loss=0.816]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 90: 100%|██████████| 7/7 [00:00<00:00, 14.51it/s, loss=0.846, v_num=24, val_loss=2.560, train_loss=0.816]\n",
      "Epoch 91:  86%|████████▌ | 6/7 [00:00<00:00, 10.03it/s, loss=0.909, v_num=24, val_loss=2.560, train_loss=0.705]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 91: 100%|██████████| 7/7 [00:00<00:00, 11.05it/s, loss=0.909, v_num=24, val_loss=2.680, train_loss=0.705]\n",
      "Epoch 92:  86%|████████▌ | 6/7 [00:00<00:00, 11.44it/s, loss=0.887, v_num=24, val_loss=2.680, train_loss=1.070]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 92: 100%|██████████| 7/7 [00:00<00:00, 12.67it/s, loss=0.887, v_num=24, val_loss=2.890, train_loss=1.070]\n",
      "Epoch 93:  86%|████████▌ | 6/7 [00:00<00:00, 11.74it/s, loss=0.827, v_num=24, val_loss=2.890, train_loss=0.628]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 93: 100%|██████████| 7/7 [00:00<00:00, 13.00it/s, loss=0.827, v_num=24, val_loss=1.550, train_loss=0.628]\n",
      "Epoch 93: 100%|██████████| 7/7 [00:00<00:00, 12.43it/s, loss=0.827, v_num=24, val_loss=1.550, train_loss=0.620]\n",
      "tensor([[[417.0000],\n",
      "         [391.0000],\n",
      "         [419.0000],\n",
      "         [461.0000],\n",
      "         [472.0000],\n",
      "         [535.0000],\n",
      "         [622.0000],\n",
      "         [606.0000],\n",
      "         [508.0000],\n",
      "         [461.0000],\n",
      "         [390.0000],\n",
      "         [432.0000]],\n",
      "\n",
      "        [[314.3913],\n",
      "         [305.7869],\n",
      "         [300.9393],\n",
      "         [293.1549],\n",
      "         [295.4145],\n",
      "         [318.7093],\n",
      "         [316.6139],\n",
      "         [312.3857],\n",
      "         [319.6606],\n",
      "         [308.3625],\n",
      "         [300.6173],\n",
      "         [284.2262]]], dtype=torch.float64)\n",
      "MAE is: tensor(170.3115, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model = Autoformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([model.transform_output(prediction=y, target_scale=x['target_scale'])\n",
    "                     for x, y in iter(val_dataloader)])\n",
    "predictions, x_index = best_model.predict(val_dataloader)\n",
    "mae = (actuals - predictions).abs().mean()\n",
    "# print('predictions shape is:', predictions.shape)\n",
    "# print('actuals shape is:', actuals.shape)\n",
    "print(torch.cat([actuals, predictions]))\n",
    "print('MAE is:', mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
