{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Informer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install deepts_forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from deepts_forecasting.utils.data import TimeSeriesDataSet\n",
    "from deepts_forecasting.utils.data.encoders import TorchNormalizer\n",
    "from deepts_forecasting.datasets import AirPassengersDataset\n",
    "from deepts_forecasting.models.informer import Informer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Passengers</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>group</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-01-01</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-02-01</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949-03-01</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949-04-01</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949-05-01</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Month  Passengers  year month group  time_idx\n",
       "0 1949-01-01       112.0  1949     1     0         0\n",
       "1 1949-02-01       118.0  1949     2     0         1\n",
       "2 1949-03-01       132.0  1949     3     0         2\n",
       "3 1949-04-01       129.0  1949     4     0         3\n",
       "4 1949-05-01       121.0  1949     5     0         4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = AirPassengersDataset().load()\n",
    "data['year'] = data['Month'].dt.year\n",
    "data['month'] = data['Month'].dt.month\n",
    "data['group'] = '0'\n",
    "data['time_idx'] = np.arange(len(data))\n",
    "data['Passengers'] = data['Passengers'].astype(float)\n",
    "data['month'] = data['month'].astype('str')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_encoder_length = 18\n",
    "max_prediction_length = 12\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_encoder_length - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    max_encoder_length= max_encoder_length,\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Passengers\",\n",
    "    group_ids=[\"group\"],\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=['month'],\n",
    "    time_varying_known_reals=[],\n",
    "    time_varying_unknown_reals=[\"Passengers\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    target_normalizer=TorchNormalizer(method=\"standard\",\n",
    "                                      transformation=None),\n",
    "    )\n",
    "\n",
    "training.get_parameters()\n",
    "validation = TimeSeriesDataSet.from_dataset(training,\n",
    "                                            data[lambda x: x.time_idx > training_cutoff])\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(training, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "val_dataloader = DataLoader(validation, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method LightningModule.summarize of Informer(\n",
       "  (loss): L1Loss()\n",
       "  (logging_metrics): ModuleList()\n",
       "  (encoder_input_linear): Linear(in_features=7, out_features=16, bias=True)\n",
       "  (decoder_input_linear): Linear(in_features=6, out_features=16, bias=True)\n",
       "  (encoder_positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder_positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (informer_encoder): InformerEncoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): InformerEncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): InformerEncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (downConv): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(2,), padding_mode=circular)\n",
       "        (norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ELU(alpha=1.0)\n",
       "        (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (informer_decoder): InformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): InformerDecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): InformerDecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_linear): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (embeddings): ModuleDict(\n",
       "    (month): Embedding(12, 6)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(1234)\n",
    "# create PyTorch Lighning Trainer with early stopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4,\n",
    "                                    patience=60, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    gpus=0,  # run on CPU, if on multiple GPUs, use accelerator=\"ddp\"\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # 30 batches per epoch\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\")\n",
    ")\n",
    "\n",
    "model = Informer.from_dataset(training,\n",
    "                                  d_model=16,\n",
    "                                  d_ff=16,\n",
    "                                  n_heads=1,\n",
    "                                  num_layers=2,\n",
    "                                      )\n",
    "model.summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"activation\":                        relu\n",
       "\"categorical_groups\":                {}\n",
       "\"d_ff\":                              16\n",
       "\"d_model\":                           16\n",
       "\"dropout\":                           0.1\n",
       "\"embedding_labels\":                  {'month': array(['1', '10', '11', '12', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
       "      dtype=object)}\n",
       "\"embedding_paddings\":                []\n",
       "\"embedding_sizes\":                   {'month': [12, 6]}\n",
       "\"learning_rate\":                     0.001\n",
       "\"log_interval\":                      -1\n",
       "\"log_val_interval\":                  None\n",
       "\"logging_metrics\":                   ModuleList()\n",
       "\"loss\":                              L1Loss()\n",
       "\"max_encoder_length\":                18\n",
       "\"max_prediction_length\":             12\n",
       "\"monotone_constaints\":               {}\n",
       "\"n_heads\":                           1\n",
       "\"num_layers\":                        2\n",
       "\"output_size\":                       1\n",
       "\"output_transformer\":                TorchNormalizer()\n",
       "\"static_categoricals\":               []\n",
       "\"static_reals\":                      []\n",
       "\"time_varying_categoricals_decoder\": ['month']\n",
       "\"time_varying_categoricals_encoder\": ['month']\n",
       "\"time_varying_reals_decoder\":        []\n",
       "\"time_varying_reals_encoder\":        ['Passengers']\n",
       "\"x_categoricals\":                    ['month']\n",
       "\"x_reals\":                           ['Passengers']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                        | Type               | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | loss                        | L1Loss             | 0     \n",
      "1 | logging_metrics             | ModuleList         | 0     \n",
      "2 | encoder_input_linear        | Linear             | 128   \n",
      "3 | decoder_input_linear        | Linear             | 112   \n",
      "4 | encoder_positional_encoding | PositionalEncoding | 0     \n",
      "5 | decoder_positional_encoding | PositionalEncoding | 0     \n",
      "6 | informer_encoder            | InformerEncoder    | 2.5 K \n",
      "7 | informer_decoder            | InformerDecoder    | 4.4 K \n",
      "8 | out_linear                  | Linear             | 17    \n",
      "9 | embeddings                  | ModuleDict         | 72    \n",
      "-------------------------------------------------------------------\n",
      "7.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  86%|████████████████████████▊    | 6/7 [00:00<00:00, 36.03it/s, loss=1.05, v_num=22]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|█████████████| 7/7 [00:00<00:00, 38.77it/s, loss=1.05, v_num=22, val_loss=2.400]\u001B[A\n",
      "Epoch 1:  86%|▊| 6/7 [00:00<00:00, 30.30it/s, loss=0.948, v_num=22, val_loss=2.400, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|█| 7/7 [00:00<00:00, 32.94it/s, loss=0.948, v_num=22, val_loss=2.320, train_loss\u001B[A\n",
      "Epoch 2:  86%|▊| 6/7 [00:00<00:00, 27.97it/s, loss=0.895, v_num=22, val_loss=2.320, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2: 100%|█| 7/7 [00:00<00:00, 30.30it/s, loss=0.895, v_num=22, val_loss=2.260, train_loss\u001B[A\n",
      "Epoch 3:  86%|▊| 6/7 [00:00<00:00, 15.81it/s, loss=0.887, v_num=22, val_loss=2.260, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3: 100%|█| 7/7 [00:00<00:00, 17.17it/s, loss=0.887, v_num=22, val_loss=2.210, train_loss\u001B[A\n",
      "Epoch 4:  86%|▊| 6/7 [00:00<00:00, 17.12it/s, loss=0.848, v_num=22, val_loss=2.210, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4: 100%|█| 7/7 [00:00<00:00, 18.59it/s, loss=0.848, v_num=22, val_loss=2.170, train_loss\u001B[A\n",
      "Epoch 5:  86%|▊| 6/7 [00:00<00:00, 14.18it/s, loss=0.829, v_num=22, val_loss=2.170, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 5: 100%|█| 7/7 [00:00<00:00, 15.57it/s, loss=0.829, v_num=22, val_loss=2.130, train_loss\u001B[A\n",
      "Epoch 6:  86%|▊| 6/7 [00:00<00:00, 16.24it/s, loss=0.815, v_num=22, val_loss=2.130, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6: 100%|█| 7/7 [00:00<00:00, 17.65it/s, loss=0.815, v_num=22, val_loss=2.090, train_loss\u001B[A\n",
      "Epoch 7:  86%|▊| 6/7 [00:00<00:00, 16.04it/s, loss=0.803, v_num=22, val_loss=2.090, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|█| 7/7 [00:00<00:00, 17.56it/s, loss=0.803, v_num=22, val_loss=2.050, train_loss\u001B[A\n",
      "Epoch 8:  86%|▊| 6/7 [00:00<00:00, 14.69it/s, loss=0.787, v_num=22, val_loss=2.050, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|█| 7/7 [00:00<00:00, 16.09it/s, loss=0.787, v_num=22, val_loss=2.000, train_loss\u001B[A\n",
      "Epoch 9:  86%|▊| 6/7 [00:00<00:00, 17.44it/s, loss=0.776, v_num=22, val_loss=2.000, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 9: 100%|█| 7/7 [00:00<00:00, 18.92it/s, loss=0.776, v_num=22, val_loss=1.960, train_loss\u001B[A\n",
      "Epoch 10:  86%|▊| 6/7 [00:00<00:00, 16.13it/s, loss=0.766, v_num=22, val_loss=1.960, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10: 100%|█| 7/7 [00:00<00:00, 17.32it/s, loss=0.766, v_num=22, val_loss=1.920, train_los\u001B[A\n",
      "Epoch 11:  86%|▊| 6/7 [00:00<00:00, 12.31it/s, loss=0.757, v_num=22, val_loss=1.920, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 11: 100%|█| 7/7 [00:00<00:00, 13.51it/s, loss=0.757, v_num=22, val_loss=1.870, train_los\u001B[A\n",
      "Epoch 12:  86%|▊| 6/7 [00:00<00:00, 12.03it/s, loss=0.748, v_num=22, val_loss=1.870, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 12: 100%|█| 7/7 [00:00<00:00, 12.98it/s, loss=0.748, v_num=22, val_loss=1.820, train_los\u001B[A\n",
      "Epoch 13:  86%|▊| 6/7 [00:00<00:00, 14.53it/s, loss=0.732, v_num=22, val_loss=1.820, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 13: 100%|█| 7/7 [00:00<00:00, 15.98it/s, loss=0.732, v_num=22, val_loss=1.750, train_los\u001B[A\n",
      "Epoch 14:  86%|▊| 6/7 [00:00<00:00, 15.13it/s, loss=0.716, v_num=22, val_loss=1.750, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14: 100%|█| 7/7 [00:00<00:00, 16.57it/s, loss=0.716, v_num=22, val_loss=1.670, train_los\u001B[A\n",
      "Epoch 15:  86%|▊| 6/7 [00:00<00:00, 17.36it/s, loss=0.694, v_num=22, val_loss=1.670, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 15: 100%|█| 7/7 [00:00<00:00, 18.94it/s, loss=0.694, v_num=22, val_loss=1.570, train_los\u001B[A\n",
      "Epoch 16:  86%|▊| 6/7 [00:00<00:00, 17.96it/s, loss=0.675, v_num=22, val_loss=1.570, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 16: 100%|█| 7/7 [00:00<00:00, 19.36it/s, loss=0.675, v_num=22, val_loss=1.470, train_los\u001B[A\n",
      "Epoch 17:  86%|▊| 6/7 [00:00<00:00, 19.86it/s, loss=0.648, v_num=22, val_loss=1.470, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17: 100%|█| 7/7 [00:00<00:00, 21.18it/s, loss=0.648, v_num=22, val_loss=1.370, train_los\u001B[A\n",
      "Epoch 18:  86%|▊| 6/7 [00:00<00:00, 19.20it/s, loss=0.616, v_num=22, val_loss=1.370, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 18: 100%|█| 7/7 [00:00<00:00, 21.05it/s, loss=0.616, v_num=22, val_loss=1.270, train_los\u001B[A\n",
      "Epoch 19:  86%|▊| 6/7 [00:00<00:00, 22.18it/s, loss=0.58, v_num=22, val_loss=1.270, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 19: 100%|█| 7/7 [00:00<00:00, 23.97it/s, loss=0.58, v_num=22, val_loss=1.160, train_loss\u001B[A\n",
      "Epoch 20:  86%|▊| 6/7 [00:00<00:00, 22.18it/s, loss=0.541, v_num=22, val_loss=1.160, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 20: 100%|█| 7/7 [00:00<00:00, 24.18it/s, loss=0.541, v_num=22, val_loss=1.090, train_los\u001B[A\n",
      "Epoch 21:  86%|▊| 6/7 [00:00<00:00, 21.89it/s, loss=0.511, v_num=22, val_loss=1.090, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 21: 100%|█| 7/7 [00:00<00:00, 23.49it/s, loss=0.511, v_num=22, val_loss=1.060, train_los\u001B[A\n",
      "Epoch 22:  86%|▊| 6/7 [00:00<00:00, 19.90it/s, loss=0.477, v_num=22, val_loss=1.060, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 22: 100%|█| 7/7 [00:00<00:00, 21.94it/s, loss=0.477, v_num=22, val_loss=1.020, train_los\u001B[A\n",
      "Epoch 23:  86%|▊| 6/7 [00:00<00:00, 23.48it/s, loss=0.443, v_num=22, val_loss=1.020, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23: 100%|█| 7/7 [00:00<00:00, 25.40it/s, loss=0.443, v_num=22, val_loss=1.070, train_los\u001B[A\n",
      "Epoch 24:  86%|▊| 6/7 [00:00<00:00, 22.98it/s, loss=0.413, v_num=22, val_loss=1.070, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24: 100%|█| 7/7 [00:00<00:00, 25.18it/s, loss=0.413, v_num=22, val_loss=1.110, train_los\u001B[A\n",
      "Epoch 25:  86%|▊| 6/7 [00:00<00:00, 21.74it/s, loss=0.382, v_num=22, val_loss=1.110, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 25: 100%|█| 7/7 [00:00<00:00, 23.60it/s, loss=0.382, v_num=22, val_loss=1.060, train_los\u001B[A\n",
      "Epoch 26:  86%|▊| 6/7 [00:00<00:00, 23.21it/s, loss=0.358, v_num=22, val_loss=1.060, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 26: 100%|█| 7/7 [00:00<00:00, 25.40it/s, loss=0.358, v_num=22, val_loss=1.090, train_los\u001B[A\n",
      "Epoch 27:  86%|▊| 6/7 [00:00<00:00, 22.68it/s, loss=0.33, v_num=22, val_loss=1.090, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 27: 100%|█| 7/7 [00:00<00:00, 24.56it/s, loss=0.33, v_num=22, val_loss=1.180, train_loss\u001B[A\n",
      "Epoch 28:  86%|▊| 6/7 [00:00<00:00, 21.54it/s, loss=0.308, v_num=22, val_loss=1.180, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 28: 100%|█| 7/7 [00:00<00:00, 23.21it/s, loss=0.308, v_num=22, val_loss=1.240, train_los\u001B[A\n",
      "Epoch 29:  86%|▊| 6/7 [00:00<00:00, 21.97it/s, loss=0.295, v_num=22, val_loss=1.240, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 29: 100%|█| 7/7 [00:00<00:00, 23.64it/s, loss=0.295, v_num=22, val_loss=1.360, train_los\u001B[A\n",
      "Epoch 30:  86%|▊| 6/7 [00:00<00:00, 23.25it/s, loss=0.282, v_num=22, val_loss=1.360, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 30: 100%|█| 7/7 [00:00<00:00, 25.09it/s, loss=0.282, v_num=22, val_loss=1.270, train_los\u001B[A\n",
      "Epoch 31:  86%|▊| 6/7 [00:00<00:00, 20.27it/s, loss=0.28, v_num=22, val_loss=1.270, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 31: 100%|█| 7/7 [00:00<00:00, 22.18it/s, loss=0.28, v_num=22, val_loss=1.360, train_loss\u001B[A\n",
      "Epoch 32:  86%|▊| 6/7 [00:00<00:00, 21.54it/s, loss=0.267, v_num=22, val_loss=1.360, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 32: 100%|█| 7/7 [00:00<00:00, 23.49it/s, loss=0.267, v_num=22, val_loss=1.330, train_los\u001B[A\n",
      "Epoch 33:  86%|▊| 6/7 [00:00<00:00, 17.49it/s, loss=0.26, v_num=22, val_loss=1.330, train_loss\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 33: 100%|█| 7/7 [00:00<00:00, 18.56it/s, loss=0.26, v_num=22, val_loss=1.460, train_loss\u001B[A\n",
      "Epoch 34:  86%|▊| 6/7 [00:00<00:00, 19.29it/s, loss=0.253, v_num=22, val_loss=1.460, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 34: 100%|█| 7/7 [00:00<00:00, 20.86it/s, loss=0.253, v_num=22, val_loss=1.520, train_los\u001B[A\n",
      "Epoch 35:  86%|▊| 6/7 [00:00<00:00, 24.19it/s, loss=0.247, v_num=22, val_loss=1.520, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 35: 100%|█| 7/7 [00:00<00:00, 26.51it/s, loss=0.247, v_num=22, val_loss=1.530, train_los\u001B[A\n",
      "Epoch 36:  86%|▊| 6/7 [00:00<00:00, 24.64it/s, loss=0.245, v_num=22, val_loss=1.530, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 36: 100%|█| 7/7 [00:00<00:00, 27.07it/s, loss=0.245, v_num=22, val_loss=1.600, train_los\u001B[A\n",
      "Epoch 37:  86%|▊| 6/7 [00:00<00:00, 24.14it/s, loss=0.238, v_num=22, val_loss=1.600, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 37: 100%|█| 7/7 [00:00<00:00, 26.26it/s, loss=0.238, v_num=22, val_loss=1.540, train_los\u001B[A\n",
      "Epoch 38:  86%|▊| 6/7 [00:00<00:00, 25.86it/s, loss=0.228, v_num=22, val_loss=1.540, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 38: 100%|█| 7/7 [00:00<00:00, 28.22it/s, loss=0.228, v_num=22, val_loss=1.400, train_los\u001B[A\n",
      "Epoch 39:  86%|▊| 6/7 [00:00<00:00, 24.04it/s, loss=0.226, v_num=22, val_loss=1.400, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 39: 100%|█| 7/7 [00:00<00:00, 25.78it/s, loss=0.226, v_num=22, val_loss=1.470, train_los\u001B[A\n",
      "Epoch 40:  86%|▊| 6/7 [00:00<00:00, 24.39it/s, loss=0.221, v_num=22, val_loss=1.470, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 40: 100%|█| 7/7 [00:00<00:00, 26.46it/s, loss=0.221, v_num=22, val_loss=1.660, train_los\u001B[A\n",
      "Epoch 41:  86%|▊| 6/7 [00:00<00:00, 25.42it/s, loss=0.221, v_num=22, val_loss=1.660, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 41: 100%|█| 7/7 [00:00<00:00, 27.72it/s, loss=0.221, v_num=22, val_loss=1.630, train_los\u001B[A\n",
      "Epoch 42:  86%|▊| 6/7 [00:00<00:00, 23.57it/s, loss=0.217, v_num=22, val_loss=1.630, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 42: 100%|█| 7/7 [00:00<00:00, 25.87it/s, loss=0.217, v_num=22, val_loss=1.600, train_los\u001B[A\n",
      "Epoch 43:  86%|▊| 6/7 [00:00<00:00, 25.15it/s, loss=0.213, v_num=22, val_loss=1.600, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 43: 100%|█| 7/7 [00:00<00:00, 27.55it/s, loss=0.213, v_num=22, val_loss=1.760, train_los\u001B[A\n",
      "Epoch 44:  86%|▊| 6/7 [00:00<00:00, 24.64it/s, loss=0.218, v_num=22, val_loss=1.760, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 44: 100%|█| 7/7 [00:00<00:00, 26.51it/s, loss=0.218, v_num=22, val_loss=1.610, train_los\u001B[A\n",
      "Epoch 45:  86%|▊| 6/7 [00:00<00:00, 24.84it/s, loss=0.217, v_num=22, val_loss=1.610, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 45: 100%|█| 7/7 [00:00<00:00, 27.23it/s, loss=0.217, v_num=22, val_loss=1.620, train_los\u001B[A\n",
      "Epoch 46:  86%|▊| 6/7 [00:00<00:00, 23.48it/s, loss=0.231, v_num=22, val_loss=1.620, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 46: 100%|█| 7/7 [00:00<00:00, 25.78it/s, loss=0.231, v_num=22, val_loss=1.710, train_los\u001B[A\n",
      "Epoch 47:  86%|▊| 6/7 [00:00<00:00, 24.64it/s, loss=0.219, v_num=22, val_loss=1.710, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 47: 100%|█| 7/7 [00:00<00:00, 26.87it/s, loss=0.219, v_num=22, val_loss=1.680, train_los\u001B[A\n",
      "Epoch 48:  86%|▊| 6/7 [00:00<00:00, 24.89it/s, loss=0.214, v_num=22, val_loss=1.680, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 48: 100%|█| 7/7 [00:00<00:00, 27.18it/s, loss=0.214, v_num=22, val_loss=1.680, train_los\u001B[A\n",
      "Epoch 49:  86%|▊| 6/7 [00:00<00:00, 25.21it/s, loss=0.204, v_num=22, val_loss=1.680, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 49: 100%|█| 7/7 [00:00<00:00, 27.55it/s, loss=0.204, v_num=22, val_loss=1.610, train_los\u001B[A\n",
      "Epoch 50:  86%|▊| 6/7 [00:00<00:00, 25.10it/s, loss=0.199, v_num=22, val_loss=1.610, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 50: 100%|█| 7/7 [00:00<00:00, 26.92it/s, loss=0.199, v_num=22, val_loss=1.640, train_los\u001B[A\n",
      "Epoch 51:  86%|▊| 6/7 [00:00<00:00, 23.71it/s, loss=0.201, v_num=22, val_loss=1.640, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 51: 100%|█| 7/7 [00:00<00:00, 25.87it/s, loss=0.201, v_num=22, val_loss=1.770, train_los\u001B[A\n",
      "Epoch 52:  86%|▊| 6/7 [00:00<00:00, 20.20it/s, loss=0.222, v_num=22, val_loss=1.770, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 52: 100%|█| 7/7 [00:00<00:00, 22.08it/s, loss=0.222, v_num=22, val_loss=1.830, train_los\u001B[A\n",
      "Epoch 53:  86%|▊| 6/7 [00:00<00:00, 26.20it/s, loss=0.218, v_num=22, val_loss=1.830, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 53: 100%|█| 7/7 [00:00<00:00, 28.51it/s, loss=0.218, v_num=22, val_loss=1.770, train_los\u001B[A\n",
      "Epoch 54:  86%|▊| 6/7 [00:00<00:00, 24.64it/s, loss=0.218, v_num=22, val_loss=1.770, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 54: 100%|█| 7/7 [00:00<00:00, 26.97it/s, loss=0.218, v_num=22, val_loss=1.910, train_los\u001B[A\n",
      "Epoch 55:  86%|▊| 6/7 [00:00<00:00, 24.34it/s, loss=0.197, v_num=22, val_loss=1.910, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 55: 100%|█| 7/7 [00:00<00:00, 26.56it/s, loss=0.197, v_num=22, val_loss=1.760, train_los\u001B[A\n",
      "Epoch 56:  86%|▊| 6/7 [00:00<00:00, 26.20it/s, loss=0.189, v_num=22, val_loss=1.760, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 56: 100%|█| 7/7 [00:00<00:00, 28.39it/s, loss=0.189, v_num=22, val_loss=1.900, train_los\u001B[A\n",
      "Epoch 57:  86%|▊| 6/7 [00:00<00:00, 23.95it/s, loss=0.187, v_num=22, val_loss=1.900, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 57: 100%|█| 7/7 [00:00<00:00, 26.11it/s, loss=0.187, v_num=22, val_loss=1.850, train_los\u001B[A\n",
      "Epoch 58:  86%|▊| 6/7 [00:00<00:00, 23.71it/s, loss=0.188, v_num=22, val_loss=1.850, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 58: 100%|█| 7/7 [00:00<00:00, 26.02it/s, loss=0.188, v_num=22, val_loss=1.930, train_los\u001B[A\n",
      "Epoch 59:  86%|▊| 6/7 [00:00<00:00, 24.49it/s, loss=0.183, v_num=22, val_loss=1.930, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 59: 100%|█| 7/7 [00:00<00:00, 26.66it/s, loss=0.183, v_num=22, val_loss=1.910, train_los\u001B[A\n",
      "Epoch 60:  86%|▊| 6/7 [00:00<00:00, 24.34it/s, loss=0.184, v_num=22, val_loss=1.910, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 60: 100%|█| 7/7 [00:00<00:00, 26.56it/s, loss=0.184, v_num=22, val_loss=1.940, train_los\u001B[A\n",
      "Epoch 61:  86%|▊| 6/7 [00:00<00:00, 24.04it/s, loss=0.185, v_num=22, val_loss=1.940, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 61: 100%|█| 7/7 [00:00<00:00, 26.36it/s, loss=0.185, v_num=22, val_loss=1.770, train_los\u001B[A\n",
      "Epoch 62:  86%|▊| 6/7 [00:00<00:00, 24.29it/s, loss=0.187, v_num=22, val_loss=1.770, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 62: 100%|█| 7/7 [00:00<00:00, 25.97it/s, loss=0.187, v_num=22, val_loss=1.890, train_los\u001B[A\n",
      "Epoch 63:  86%|▊| 6/7 [00:00<00:00, 24.09it/s, loss=0.187, v_num=22, val_loss=1.890, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 63: 100%|█| 7/7 [00:00<00:00, 26.31it/s, loss=0.187, v_num=22, val_loss=1.890, train_los\u001B[A\n",
      "Epoch 64:  86%|▊| 6/7 [00:00<00:00, 23.48it/s, loss=0.184, v_num=22, val_loss=1.890, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 64: 100%|█| 7/7 [00:00<00:00, 25.78it/s, loss=0.184, v_num=22, val_loss=1.820, train_los\u001B[A\n",
      "Epoch 65:  86%|▊| 6/7 [00:00<00:00, 23.85it/s, loss=0.188, v_num=22, val_loss=1.820, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 65: 100%|█| 7/7 [00:00<00:00, 26.16it/s, loss=0.188, v_num=22, val_loss=1.830, train_los\u001B[A\n",
      "Epoch 66:  86%|▊| 6/7 [00:00<00:00, 23.34it/s, loss=0.183, v_num=22, val_loss=1.830, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 66: 100%|█| 7/7 [00:00<00:00, 25.59it/s, loss=0.183, v_num=22, val_loss=1.950, train_los\u001B[A\n",
      "Epoch 67:  86%|▊| 6/7 [00:00<00:00, 24.94it/s, loss=0.191, v_num=22, val_loss=1.950, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 67: 100%|█| 7/7 [00:00<00:00, 27.02it/s, loss=0.191, v_num=22, val_loss=1.900, train_los\u001B[A\n",
      "Epoch 68:  86%|▊| 6/7 [00:00<00:00, 23.57it/s, loss=0.187, v_num=22, val_loss=1.900, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 68: 100%|█| 7/7 [00:00<00:00, 25.78it/s, loss=0.187, v_num=22, val_loss=1.890, train_los\u001B[A\n",
      "Epoch 69:  86%|▊| 6/7 [00:00<00:00, 23.81it/s, loss=0.184, v_num=22, val_loss=1.890, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 69: 100%|█| 7/7 [00:00<00:00, 26.02it/s, loss=0.184, v_num=22, val_loss=1.880, train_los\u001B[A\n",
      "Epoch 70:  86%|▊| 6/7 [00:00<00:00, 25.00it/s, loss=0.175, v_num=22, val_loss=1.880, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 70: 100%|█| 7/7 [00:00<00:00, 27.23it/s, loss=0.175, v_num=22, val_loss=1.980, train_los\u001B[A\n",
      "Epoch 71:  86%|▊| 6/7 [00:00<00:00, 24.74it/s, loss=0.186, v_num=22, val_loss=1.980, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 71: 100%|█| 7/7 [00:00<00:00, 27.13it/s, loss=0.186, v_num=22, val_loss=2.020, train_los\u001B[A\n",
      "Epoch 72:  86%|▊| 6/7 [00:00<00:00, 22.60it/s, loss=0.186, v_num=22, val_loss=2.020, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 72: 100%|█| 7/7 [00:00<00:00, 24.56it/s, loss=0.186, v_num=22, val_loss=1.890, train_los\u001B[A\n",
      "Epoch 73:  86%|▊| 6/7 [00:00<00:00, 22.64it/s, loss=0.186, v_num=22, val_loss=1.890, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 73: 100%|█| 7/7 [00:00<00:00, 24.56it/s, loss=0.186, v_num=22, val_loss=2.030, train_los\u001B[A\n",
      "Epoch 74:  86%|▊| 6/7 [00:00<00:00, 24.94it/s, loss=0.173, v_num=22, val_loss=2.030, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 74: 100%|█| 7/7 [00:00<00:00, 27.34it/s, loss=0.173, v_num=22, val_loss=1.920, train_los\u001B[A\n",
      "Epoch 75:  86%|▊| 6/7 [00:00<00:00, 22.77it/s, loss=0.173, v_num=22, val_loss=1.920, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 75: 100%|█| 7/7 [00:00<00:00, 25.04it/s, loss=0.173, v_num=22, val_loss=1.780, train_los\u001B[A\n",
      "Epoch 76:  86%|▊| 6/7 [00:00<00:00, 21.58it/s, loss=0.169, v_num=22, val_loss=1.780, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 76: 100%|█| 7/7 [00:00<00:00, 23.10it/s, loss=0.169, v_num=22, val_loss=2.080, train_los\u001B[A\n",
      "Epoch 77:  86%|▊| 6/7 [00:00<00:00, 24.94it/s, loss=0.175, v_num=22, val_loss=2.080, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 77: 100%|█| 7/7 [00:00<00:00, 27.34it/s, loss=0.175, v_num=22, val_loss=2.040, train_los\u001B[A\n",
      "Epoch 78:  86%|▊| 6/7 [00:00<00:00, 23.62it/s, loss=0.175, v_num=22, val_loss=2.040, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 78: 100%|█| 7/7 [00:00<00:00, 25.68it/s, loss=0.175, v_num=22, val_loss=1.880, train_los\u001B[A\n",
      "Epoch 79:  86%|▊| 6/7 [00:00<00:00, 24.64it/s, loss=0.182, v_num=22, val_loss=1.880, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 79: 100%|█| 7/7 [00:00<00:00, 26.92it/s, loss=0.182, v_num=22, val_loss=1.900, train_los\u001B[A\n",
      "Epoch 80:  86%|▊| 6/7 [00:00<00:00, 23.39it/s, loss=0.174, v_num=22, val_loss=1.900, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 80: 100%|█| 7/7 [00:00<00:00, 25.54it/s, loss=0.174, v_num=22, val_loss=2.020, train_los\u001B[A\n",
      "Epoch 81:  86%|▊| 6/7 [00:00<00:00, 24.09it/s, loss=0.171, v_num=22, val_loss=2.020, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 81: 100%|█| 7/7 [00:00<00:00, 26.36it/s, loss=0.171, v_num=22, val_loss=2.060, train_los\u001B[A\n",
      "Epoch 82:  86%|▊| 6/7 [00:00<00:00, 23.76it/s, loss=0.161, v_num=22, val_loss=2.060, train_los\u001B[A\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 82: 100%|█| 7/7 [00:00<00:00, 25.83it/s, loss=0.161, v_num=22, val_loss=2.060, train_los\u001B[A\n",
      "Epoch 82: 100%|█| 7/7 [00:00<00:00, 23.72it/s, loss=0.161, v_num=22, val_loss=2.060, train_los\u001B[A\n",
      "tensor([[[417.0000],\n",
      "         [391.0000],\n",
      "         [419.0000],\n",
      "         [461.0000],\n",
      "         [472.0000],\n",
      "         [535.0000],\n",
      "         [622.0000],\n",
      "         [606.0000],\n",
      "         [508.0000],\n",
      "         [461.0000],\n",
      "         [390.0000],\n",
      "         [432.0000]],\n",
      "\n",
      "        [[242.2041],\n",
      "         [248.4361],\n",
      "         [291.6753],\n",
      "         [285.4878],\n",
      "         [300.7017],\n",
      "         [335.5658],\n",
      "         [392.4390],\n",
      "         [390.4636],\n",
      "         [305.8913],\n",
      "         [259.6950],\n",
      "         [234.0442],\n",
      "         [245.8051]]], dtype=torch.float64)\n",
      "MAE is: tensor(181.7993, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model = Informer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([model.transform_output(prediction=y, target_scale=x['target_scale'])\n",
    "                     for x, y in iter(val_dataloader)])\n",
    "predictions, x_index = best_model.predict(val_dataloader)\n",
    "mae = (actuals - predictions).abs().mean()\n",
    "# print('predictions shape is:', predictions.shape)\n",
    "# print('actuals shape is:', actuals.shape)\n",
    "print(torch.cat([actuals, predictions]))\n",
    "print('MAE is:', mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}