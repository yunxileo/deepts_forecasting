{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Informer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install deepts_forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from deepts_forecasting.utils.data import TimeSeriesDataSet\n",
    "from deepts_forecasting.utils.data.encoders import TorchNormalizer\n",
    "from deepts_forecasting.datasets import AirPassengersDataset\n",
    "from deepts_forecasting.models.informer.informer import Informer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Passengers</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>group</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-01-01</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-02-01</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949-03-01</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949-04-01</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949-05-01</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Month  Passengers  year month group  time_idx\n",
       "0 1949-01-01       112.0  1949     1     0         0\n",
       "1 1949-02-01       118.0  1949     2     0         1\n",
       "2 1949-03-01       132.0  1949     3     0         2\n",
       "3 1949-04-01       129.0  1949     4     0         3\n",
       "4 1949-05-01       121.0  1949     5     0         4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = AirPassengersDataset().load()\n",
    "data['year'] = data['Month'].dt.year\n",
    "data['month'] = data['Month'].dt.month\n",
    "data['group'] = '0'\n",
    "data['time_idx'] = np.arange(len(data))\n",
    "data['Passengers'] = data['Passengers'].astype(float)\n",
    "data['month'] = data['month'].astype('str')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 18\n",
    "max_prediction_length = 12\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_encoder_length - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    max_encoder_length= max_encoder_length,\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Passengers\",\n",
    "    group_ids=[\"group\"],\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=['month'],\n",
    "    time_varying_known_reals=[],\n",
    "    time_varying_unknown_reals=[\"Passengers\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    target_normalizer=TorchNormalizer(method=\"standard\",\n",
    "                                      transformation=None),\n",
    "    )\n",
    "\n",
    "training.get_parameters()\n",
    "validation = TimeSeriesDataSet.from_dataset(training,\n",
    "                                            data[lambda x: x.time_idx > training_cutoff])\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(training, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "val_dataloader = DataLoader(validation, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method LightningModule.summarize of Informer(\n",
       "  (loss): MSELoss()\n",
       "  (logging_metrics): ModuleList()\n",
       "  (encoder_input_linear): Linear(in_features=7, out_features=16, bias=True)\n",
       "  (decoder_input_linear): Linear(in_features=6, out_features=16, bias=True)\n",
       "  (encoder_positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder_positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (informer_encoder): InformerEncoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): InformerEncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): InformerEncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (downConv): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(2,), padding_mode=circular)\n",
       "        (norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ELU(alpha=1.0)\n",
       "        (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (informer_decoder): InformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): InformerDecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): InformerDecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_linear): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (embeddings): ModuleDict(\n",
       "    (month): Embedding(12, 6)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(1234)\n",
    "# create PyTorch Lighning Trainer with early stopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4,\n",
    "                                    patience=60, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    gpus=0,  # run on CPU, if on multiple GPUs, use accelerator=\"ddp\"\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # 30 batches per epoch\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\")\n",
    ")\n",
    "\n",
    "model = Informer.from_dataset(training,\n",
    "                                  d_model=16,\n",
    "                                  d_ff=32,\n",
    "                                  n_heads=2,\n",
    "                                  num_layers=2,\n",
    "                                      )\n",
    "model.summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"activation\":                        relu\n",
       "\"categorical_groups\":                {}\n",
       "\"d_ff\":                              32\n",
       "\"d_model\":                           16\n",
       "\"dropout\":                           0.1\n",
       "\"embedding_labels\":                  {'month': array(['1', '10', '11', '12', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
       "      dtype=object)}\n",
       "\"embedding_paddings\":                []\n",
       "\"embedding_sizes\":                   {'month': [12, 6]}\n",
       "\"learning_rate\":                     0.001\n",
       "\"log_interval\":                      -1\n",
       "\"log_val_interval\":                  None\n",
       "\"logging_metrics\":                   ModuleList()\n",
       "\"loss\":                              L1Loss()\n",
       "\"max_encoder_length\":                18\n",
       "\"max_prediction_length\":             12\n",
       "\"monotone_constaints\":               {}\n",
       "\"n_heads\":                           2\n",
       "\"num_layers\":                        2\n",
       "\"output_size\":                       1\n",
       "\"output_transformer\":                TorchNormalizer()\n",
       "\"static_categoricals\":               []\n",
       "\"static_reals\":                      []\n",
       "\"time_varying_categoricals_decoder\": ['month']\n",
       "\"time_varying_categoricals_encoder\": ['month']\n",
       "\"time_varying_reals_decoder\":        []\n",
       "\"time_varying_reals_encoder\":        ['Passengers']\n",
       "\"x_categoricals\":                    ['month']\n",
       "\"x_reals\":                           ['Passengers']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name                        | Type               | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | loss                        | MSELoss            | 0     \n",
      "1 | logging_metrics             | ModuleList         | 0     \n",
      "2 | encoder_input_linear        | Linear             | 128   \n",
      "3 | decoder_input_linear        | Linear             | 112   \n",
      "4 | encoder_positional_encoding | PositionalEncoding | 0     \n",
      "5 | decoder_positional_encoding | PositionalEncoding | 0     \n",
      "6 | informer_encoder            | InformerEncoder    | 3.1 K \n",
      "7 | informer_decoder            | InformerDecoder    | 4.4 K \n",
      "8 | out_linear                  | Linear             | 17    \n",
      "9 | embeddings                  | ModuleDict         | 72    \n",
      "-------------------------------------------------------------------\n",
      "7.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 1234\n",
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "D:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:432: UserWarning: The number of training samples (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  86%|██████████▎ | 6/7 [00:00<00:00, 33.05it/s, loss=1.53, v_num=20]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 7/7 [00:00<00:00, 35.71it/s, loss=1.53, v_num=20, val_loss=9\u001b[A\n",
      "Epoch 1:  86%|▊| 6/7 [00:00<00:00, 28.84it/s, loss=1.23, v_num=20, val_loss=9\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 7/7 [00:00<00:00, 31.60it/s, loss=1.23, v_num=20, val_loss=7\u001b[A\n",
      "Epoch 2:  86%|▊| 6/7 [00:00<00:00, 24.39it/s, loss=1.11, v_num=20, val_loss=7\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 7/7 [00:00<00:00, 25.83it/s, loss=1.11, v_num=20, val_loss=7\u001b[A\n",
      "Epoch 3:  86%|▊| 6/7 [00:00<00:00, 16.66it/s, loss=1.11, v_num=20, val_loss=7\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 7/7 [00:00<00:00, 18.06it/s, loss=1.11, v_num=20, val_loss=7\u001b[A\n",
      "Epoch 4:  86%|▊| 6/7 [00:00<00:00, 15.95it/s, loss=0.956, v_num=20, val_loss=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 7/7 [00:00<00:00, 17.07it/s, loss=0.956, v_num=20, val_loss=\u001b[A\n",
      "Epoch 5:  86%|▊| 6/7 [00:00<00:00, 15.34it/s, loss=0.9, v_num=20, val_loss=6.\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 7/7 [00:00<00:00, 16.72it/s, loss=0.9, v_num=20, val_loss=6.\u001b[A\n",
      "Epoch 6:  86%|▊| 6/7 [00:00<00:00, 15.64it/s, loss=0.871, v_num=20, val_loss=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 7/7 [00:00<00:00, 16.82it/s, loss=0.871, v_num=20, val_loss=\u001b[A\n",
      "Epoch 7:  86%|▊| 6/7 [00:00<00:00, 16.62it/s, loss=0.846, v_num=20, val_loss=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 7/7 [00:00<00:00, 17.72it/s, loss=0.846, v_num=20, val_loss=\u001b[A\n",
      "Epoch 8:  86%|▊| 6/7 [00:00<00:00, 13.77it/s, loss=0.833, v_num=20, val_loss=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 7/7 [00:00<00:00, 15.02it/s, loss=0.833, v_num=20, val_loss=\u001b[A\n",
      "Epoch 9:  86%|▊| 6/7 [00:00<00:00, 14.98it/s, loss=0.822, v_num=20, val_loss=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 7/7 [00:00<00:00, 16.47it/s, loss=0.822, v_num=20, val_loss=\u001b[A\n",
      "Epoch 10:  86%|▊| 6/7 [00:00<00:00, 16.24it/s, loss=0.808, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 7/7 [00:00<00:00, 17.70it/s, loss=0.808, v_num=20, val_loss\u001b[A\n",
      "Epoch 11:  86%|▊| 6/7 [00:00<00:00, 16.37it/s, loss=0.803, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 7/7 [00:00<00:00, 17.72it/s, loss=0.803, v_num=20, val_loss\u001b[A\n",
      "Epoch 12:  86%|▊| 6/7 [00:00<00:00, 17.41it/s, loss=0.79, v_num=20, val_loss=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 7/7 [00:00<00:00, 18.97it/s, loss=0.79, v_num=20, val_loss=\u001b[A\n",
      "Epoch 13:  86%|▊| 6/7 [00:00<00:00, 19.04it/s, loss=0.767, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 7/7 [00:00<00:00, 20.89it/s, loss=0.767, v_num=20, val_loss\u001b[A\n",
      "Epoch 14:  86%|▊| 6/7 [00:00<00:00, 20.65it/s, loss=0.736, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 7/7 [00:00<00:00, 22.69it/s, loss=0.736, v_num=20, val_loss\u001b[A\n",
      "Epoch 15:  86%|▊| 6/7 [00:00<00:00, 20.98it/s, loss=0.704, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 7/7 [00:00<00:00, 22.69it/s, loss=0.704, v_num=20, val_loss\u001b[A\n",
      "Epoch 16:  86%|▊| 6/7 [00:00<00:00, 16.35it/s, loss=0.673, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 7/7 [00:00<00:00, 17.90it/s, loss=0.673, v_num=20, val_loss\u001b[A\n",
      "Epoch 17:  86%|▊| 6/7 [00:00<00:00, 17.67it/s, loss=0.634, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 7/7 [00:00<00:00, 19.33it/s, loss=0.634, v_num=20, val_loss\u001b[A\n",
      "Epoch 18: 100%|█| 7/7 [00:00<00:00, 25.68it/s, loss=0.597, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 7/7 [00:00<00:00, 24.13it/s, loss=0.597, v_num=20, val_loss\u001b[A\n",
      "Epoch 19:  86%|▊| 6/7 [00:00<00:00, 25.10it/s, loss=0.558, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 7/7 [00:00<00:00, 27.34it/s, loss=0.558, v_num=20, val_loss\u001b[A\n",
      "Epoch 20:  86%|▊| 6/7 [00:00<00:00, 24.39it/s, loss=0.524, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 7/7 [00:00<00:00, 26.41it/s, loss=0.524, v_num=20, val_loss\u001b[A\n",
      "Epoch 21:  86%|▊| 6/7 [00:00<00:00, 25.47it/s, loss=0.494, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 7/7 [00:00<00:00, 27.39it/s, loss=0.494, v_num=20, val_loss\u001b[A\n",
      "Epoch 22:  86%|▊| 6/7 [00:00<00:00, 22.98it/s, loss=0.451, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 7/7 [00:00<00:00, 25.00it/s, loss=0.451, v_num=20, val_loss\u001b[A\n",
      "Epoch 23:  86%|▊| 6/7 [00:00<00:00, 24.09it/s, loss=0.421, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 7/7 [00:00<00:00, 26.07it/s, loss=0.421, v_num=20, val_loss\u001b[A\n",
      "Epoch 24:  86%|▊| 6/7 [00:00<00:00, 24.29it/s, loss=0.399, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 7/7 [00:00<00:00, 26.51it/s, loss=0.399, v_num=20, val_loss\u001b[A\n",
      "Epoch 25:  86%|▊| 6/7 [00:00<00:00, 26.60it/s, loss=0.362, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 7/7 [00:00<00:00, 28.80it/s, loss=0.362, v_num=20, val_loss\u001b[A\n",
      "Epoch 26:  86%|▊| 6/7 [00:00<00:00, 24.69it/s, loss=0.333, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 7/7 [00:00<00:00, 27.02it/s, loss=0.333, v_num=20, val_loss\u001b[A\n",
      "Epoch 27:  86%|▊| 6/7 [00:00<00:00, 24.54it/s, loss=0.312, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 7/7 [00:00<00:00, 26.82it/s, loss=0.312, v_num=20, val_loss\u001b[A\n",
      "Epoch 28:  86%|▊| 6/7 [00:00<00:00, 22.55it/s, loss=0.278, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 7/7 [00:00<00:00, 24.77it/s, loss=0.278, v_num=20, val_loss\u001b[A\n",
      "Epoch 29:  86%|▊| 6/7 [00:00<00:00, 23.66it/s, loss=0.247, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 7/7 [00:00<00:00, 25.68it/s, loss=0.247, v_num=20, val_loss\u001b[A\n",
      "Epoch 30:  86%|▊| 6/7 [00:00<00:00, 23.48it/s, loss=0.231, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 7/7 [00:00<00:00, 25.64it/s, loss=0.231, v_num=20, val_loss\u001b[A\n",
      "Epoch 31:  86%|▊| 6/7 [00:00<00:00, 20.58it/s, loss=0.203, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 7/7 [00:00<00:00, 22.29it/s, loss=0.203, v_num=20, val_loss\u001b[A\n",
      "Epoch 32:  86%|▊| 6/7 [00:00<00:00, 19.51it/s, loss=0.183, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 7/7 [00:00<00:00, 21.50it/s, loss=0.183, v_num=20, val_loss\u001b[A\n",
      "Epoch 33:  86%|▊| 6/7 [00:00<00:00, 23.85it/s, loss=0.154, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 7/7 [00:00<00:00, 25.22it/s, loss=0.154, v_num=20, val_loss\u001b[A\n",
      "Epoch 34:  86%|▊| 6/7 [00:00<00:00, 21.27it/s, loss=0.137, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 7/7 [00:00<00:00, 23.10it/s, loss=0.137, v_num=20, val_loss\u001b[A\n",
      "Epoch 35:  86%|▊| 6/7 [00:00<00:00, 23.43it/s, loss=0.122, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 7/7 [00:00<00:00, 25.31it/s, loss=0.122, v_num=20, val_loss\u001b[A\n",
      "Epoch 36:  86%|▊| 6/7 [00:00<00:00, 23.57it/s, loss=0.108, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 7/7 [00:00<00:00, 25.36it/s, loss=0.108, v_num=20, val_loss\u001b[A\n",
      "Epoch 37:  86%|▊| 6/7 [00:00<00:00, 22.85it/s, loss=0.104, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 7/7 [00:00<00:00, 24.82it/s, loss=0.104, v_num=20, val_loss\u001b[A\n",
      "Epoch 38:  86%|▊| 6/7 [00:00<00:00, 22.05it/s, loss=0.0989, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 7/7 [00:00<00:00, 24.05it/s, loss=0.0989, v_num=20, val_los\u001b[A\n",
      "Epoch 39:  86%|▊| 6/7 [00:00<00:00, 22.94it/s, loss=0.0934, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 7/7 [00:00<00:00, 25.18it/s, loss=0.0934, v_num=20, val_los\u001b[A\n",
      "Epoch 40:  86%|▊| 6/7 [00:00<00:00, 28.36it/s, loss=0.087, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 7/7 [00:00<00:00, 30.70it/s, loss=0.087, v_num=20, val_loss\u001b[A\n",
      "Epoch 41:  86%|▊| 6/7 [00:00<00:00, 28.43it/s, loss=0.0794, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 7/7 [00:00<00:00, 30.76it/s, loss=0.0794, v_num=20, val_los\u001b[A\n",
      "Epoch 42:  86%|▊| 6/7 [00:00<00:00, 23.21it/s, loss=0.0862, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 7/7 [00:00<00:00, 25.45it/s, loss=0.0862, v_num=20, val_los\u001b[A\n",
      "Epoch 43:  86%|▊| 6/7 [00:00<00:00, 22.18it/s, loss=0.0834, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 7/7 [00:00<00:00, 24.30it/s, loss=0.0834, v_num=20, val_los\u001b[A\n",
      "Epoch 44:  86%|▊| 6/7 [00:00<00:00, 26.78it/s, loss=0.0768, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 7/7 [00:00<00:00, 28.68it/s, loss=0.0768, v_num=20, val_los\u001b[A\n",
      "Epoch 45:  86%|▊| 6/7 [00:00<00:00, 23.12it/s, loss=0.0713, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 7/7 [00:00<00:00, 25.40it/s, loss=0.0713, v_num=20, val_los\u001b[A\n",
      "Epoch 46:  86%|▊| 6/7 [00:00<00:00, 27.46it/s, loss=0.0662, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 7/7 [00:00<00:00, 29.91it/s, loss=0.0662, v_num=20, val_los\u001b[A\n",
      "Epoch 47:  86%|▊| 6/7 [00:00<00:00, 26.90it/s, loss=0.0694, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 7/7 [00:00<00:00, 29.22it/s, loss=0.0694, v_num=20, val_los\u001b[A\n",
      "Epoch 48:  86%|▊| 6/7 [00:00<00:00, 27.21it/s, loss=0.0673, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 7/7 [00:00<00:00, 29.41it/s, loss=0.0673, v_num=20, val_los\u001b[A\n",
      "Epoch 49:  86%|▊| 6/7 [00:00<00:00, 26.43it/s, loss=0.0658, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 7/7 [00:00<00:00, 28.68it/s, loss=0.0658, v_num=20, val_los\u001b[A\n",
      "Epoch 50:  86%|▊| 6/7 [00:00<00:00, 26.90it/s, loss=0.0605, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 7/7 [00:00<00:00, 29.28it/s, loss=0.0605, v_num=20, val_los\u001b[A\n",
      "Epoch 51:  86%|▊| 6/7 [00:00<00:00, 26.49it/s, loss=0.0589, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 7/7 [00:00<00:00, 28.51it/s, loss=0.0589, v_num=20, val_los\u001b[A\n",
      "Epoch 52:  86%|▊| 6/7 [00:00<00:00, 22.01it/s, loss=0.0572, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 7/7 [00:00<00:00, 24.13it/s, loss=0.0572, v_num=20, val_los\u001b[A\n",
      "Epoch 53:  86%|▊| 6/7 [00:00<00:00, 24.94it/s, loss=0.0568, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 7/7 [00:00<00:00, 26.76it/s, loss=0.0568, v_num=20, val_los\u001b[A\n",
      "Epoch 54:  86%|▊| 6/7 [00:00<00:00, 25.75it/s, loss=0.0574, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 7/7 [00:00<00:00, 27.88it/s, loss=0.0574, v_num=20, val_los\u001b[A\n",
      "Epoch 55:  86%|▊| 6/7 [00:00<00:00, 26.84it/s, loss=0.0581, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 7/7 [00:00<00:00, 29.10it/s, loss=0.0581, v_num=20, val_los\u001b[A\n",
      "Epoch 56:  86%|▊| 6/7 [00:00<00:00, 28.43it/s, loss=0.0629, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 7/7 [00:00<00:00, 30.56it/s, loss=0.0629, v_num=20, val_los\u001b[A\n",
      "Epoch 57:  86%|▊| 6/7 [00:00<00:00, 21.20it/s, loss=0.0593, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 7/7 [00:00<00:00, 23.25it/s, loss=0.0593, v_num=20, val_los\u001b[A\n",
      "Epoch 58:  86%|▊| 6/7 [00:00<00:00, 27.71it/s, loss=0.0551, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 7/7 [00:00<00:00, 30.04it/s, loss=0.0551, v_num=20, val_los\u001b[A\n",
      "Epoch 59:  86%|▊| 6/7 [00:00<00:00, 27.33it/s, loss=0.0513, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 7/7 [00:00<00:00, 29.78it/s, loss=0.0513, v_num=20, val_los\u001b[A\n",
      "Epoch 60:  86%|▊| 6/7 [00:00<00:00, 22.77it/s, loss=0.0482, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 7/7 [00:00<00:00, 24.47it/s, loss=0.0482, v_num=20, val_los\u001b[A\n",
      "Epoch 61:  86%|▊| 6/7 [00:00<00:00, 22.85it/s, loss=0.0524, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 7/7 [00:00<00:00, 25.00it/s, loss=0.0524, v_num=20, val_los\u001b[A\n",
      "Epoch 62:  86%|▊| 6/7 [00:00<00:00, 26.60it/s, loss=0.0519, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 7/7 [00:00<00:00, 28.86it/s, loss=0.0519, v_num=20, val_los\u001b[A\n",
      "Epoch 63:  86%|▊| 6/7 [00:00<00:00, 21.20it/s, loss=0.0507, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 7/7 [00:00<00:00, 23.17it/s, loss=0.0507, v_num=20, val_los\u001b[A\n",
      "Epoch 64:  86%|▊| 6/7 [00:00<00:00, 25.80it/s, loss=0.0447, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 7/7 [00:00<00:00, 28.00it/s, loss=0.0447, v_num=20, val_los\u001b[A\n",
      "Epoch 65:  86%|▊| 6/7 [00:00<00:00, 20.37it/s, loss=0.0429, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 7/7 [00:00<00:00, 22.04it/s, loss=0.0429, v_num=20, val_los\u001b[A\n",
      "Epoch 66:  86%|▊| 6/7 [00:00<00:00, 23.43it/s, loss=0.0425, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 7/7 [00:00<00:00, 25.54it/s, loss=0.0425, v_num=20, val_los\u001b[A\n",
      "Epoch 67:  86%|▊| 6/7 [00:00<00:00, 22.38it/s, loss=0.0428, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|█| 7/7 [00:00<00:00, 24.30it/s, loss=0.0428, v_num=20, val_los\u001b[A\n",
      "Epoch 68:  86%|▊| 6/7 [00:00<00:00, 20.87it/s, loss=0.0423, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|█| 7/7 [00:00<00:00, 22.61it/s, loss=0.0423, v_num=20, val_los\u001b[A\n",
      "Epoch 69:  86%|▊| 6/7 [00:00<00:00, 20.61it/s, loss=0.042, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|█| 7/7 [00:00<00:00, 22.36it/s, loss=0.042, v_num=20, val_loss\u001b[A\n",
      "Epoch 70:  86%|▊| 6/7 [00:00<00:00, 19.64it/s, loss=0.0421, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|█| 7/7 [00:00<00:00, 21.24it/s, loss=0.0421, v_num=20, val_los\u001b[A\n",
      "Epoch 71:  86%|▊| 6/7 [00:00<00:00, 15.85it/s, loss=0.0399, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|█| 7/7 [00:00<00:00, 17.32it/s, loss=0.0399, v_num=20, val_los\u001b[A\n",
      "Epoch 72:  86%|▊| 6/7 [00:00<00:00, 18.46it/s, loss=0.043, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|█| 7/7 [00:00<00:00, 20.14it/s, loss=0.043, v_num=20, val_loss\u001b[A\n",
      "Epoch 73:  86%|▊| 6/7 [00:00<00:00, 18.89it/s, loss=0.044, v_num=20, val_loss\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|█| 7/7 [00:00<00:00, 20.29it/s, loss=0.044, v_num=20, val_loss\u001b[A\n",
      "Epoch 74:  86%|▊| 6/7 [00:00<00:00, 21.62it/s, loss=0.0462, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|█| 7/7 [00:00<00:00, 23.56it/s, loss=0.0462, v_num=20, val_los\u001b[A\n",
      "Epoch 75:  86%|▊| 6/7 [00:00<00:00, 23.07it/s, loss=0.0422, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|█| 7/7 [00:00<00:00, 24.91it/s, loss=0.0422, v_num=20, val_los\u001b[A\n",
      "Epoch 76:  86%|▊| 6/7 [00:00<00:00, 23.53it/s, loss=0.0418, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|█| 7/7 [00:00<00:00, 25.68it/s, loss=0.0418, v_num=20, val_los\u001b[A\n",
      "Epoch 77:  86%|▊| 6/7 [00:00<00:00, 23.62it/s, loss=0.0413, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 77: 100%|█| 7/7 [00:00<00:00, 25.68it/s, loss=0.0413, v_num=20, val_los\u001b[A\n",
      "Epoch 78:  86%|▊| 6/7 [00:00<00:00, 21.54it/s, loss=0.0414, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 78: 100%|█| 7/7 [00:00<00:00, 23.68it/s, loss=0.0414, v_num=20, val_los\u001b[A\n",
      "Epoch 79:  86%|▊| 6/7 [00:00<00:00, 22.72it/s, loss=0.0409, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 79: 100%|█| 7/7 [00:00<00:00, 24.82it/s, loss=0.0409, v_num=20, val_los\u001b[A\n",
      "Epoch 80:  86%|▊| 6/7 [00:00<00:00, 22.43it/s, loss=0.0404, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 80: 100%|█| 7/7 [00:00<00:00, 24.47it/s, loss=0.0404, v_num=20, val_los\u001b[A\n",
      "Epoch 81:  86%|▊| 6/7 [00:00<00:00, 22.10it/s, loss=0.0411, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 81: 100%|█| 7/7 [00:00<00:00, 23.97it/s, loss=0.0411, v_num=20, val_los\u001b[A\n",
      "Epoch 82:  86%|▊| 6/7 [00:00<00:00, 22.14it/s, loss=0.0403, v_num=20, val_los\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 82: 100%|█| 7/7 [00:00<00:00, 23.72it/s, loss=0.0403, v_num=20, val_los\u001b[A\n",
      "Epoch 82: 100%|█| 7/7 [00:00<00:00, 21.94it/s, loss=0.0403, v_num=20, val_los\u001b[A\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m best_model \u001b[38;5;241m=\u001b[39m Informer\u001b[38;5;241m.\u001b[39mload_from_checkpoint(best_model_path)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# calcualte mean absolute error on validation set\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m actuals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([model\u001b[38;5;241m.\u001b[39mtransform_output(prediction\u001b[38;5;241m=\u001b[39my, target_scale\u001b[38;5;241m=\u001b[39mx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_scale\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(val_dataloader)])\n\u001b[0;32m     12\u001b[0m predictions, x_index \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(val_dataloader)\n\u001b[0;32m     13\u001b[0m mae \u001b[38;5;241m=\u001b[39m (actuals \u001b[38;5;241m-\u001b[39m predictions)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m best_model \u001b[38;5;241m=\u001b[39m Informer\u001b[38;5;241m.\u001b[39mload_from_checkpoint(best_model_path)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# calcualte mean absolute error on validation set\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m actuals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_scale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(val_dataloader)])\n\u001b[0;32m     12\u001b[0m predictions, x_index \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(val_dataloader)\n\u001b[0;32m     13\u001b[0m mae \u001b[38;5;241m=\u001b[39m (actuals \u001b[38;5;241m-\u001b[39m predictions)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\DeepTS_Forecasting\\lib\\site-packages\\deepts_forecasting\\models\\base_model.py:215\u001b[0m, in \u001b[0;36mBaseModel.transform_output\u001b[1;34m(self, prediction, target_scale)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_output\u001b[39m(\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    202\u001b[0m     prediction: Union[torch\u001b[38;5;241m.\u001b[39mTensor, List[torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[0;32m    203\u001b[0m     target_scale: Union[torch\u001b[38;5;241m.\u001b[39mTensor, List[torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[0;32m    204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    Extract prediction from network output and rescale it to real space / de-normalize it.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m        torch.Tensor: rescaled prediction\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model = Informer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([model.transform_output(prediction=y, target_scale=x['target_scale'])\n",
    "                     for x, y in iter(val_dataloader)])\n",
    "predictions, x_index = best_model.predict(val_dataloader)\n",
    "mae = (actuals - predictions).abs().mean()\n",
    "# print('predictions shape is:', predictions.shape)\n",
    "# print('actuals shape is:', actuals.shape)\n",
    "print(torch.cat([actuals, predictions]))\n",
    "print('MAE is:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform_output() missing 2 required positional arguments: 'prediction' and 'target_scale'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: transform_output() missing 2 required positional arguments: 'prediction' and 'target_scale'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[237.7456,  88.2423]]], dtype=torch.float64) tensor([[[2.0314],\n",
      "         [1.7367],\n",
      "         [2.0541],\n",
      "         [2.5300],\n",
      "         [2.6547],\n",
      "         [3.3686],\n",
      "         [4.3545],\n",
      "         [4.1732],\n",
      "         [3.0626],\n",
      "         [2.5300],\n",
      "         [1.7254],\n",
      "         [2.2014]]])\n"
     ]
    }
   ],
   "source": [
    " for x, y in iter(val_dataloader):\n",
    "    print(x['target_scale'],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
